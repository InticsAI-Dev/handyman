package in.handyman.raven.lib;

import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import in.handyman.raven.exception.HandymanException;
import in.handyman.raven.lambda.access.ResourceAccess;
import in.handyman.raven.lambda.action.ActionExecution;
import in.handyman.raven.lambda.action.IActionExecution;
import in.handyman.raven.lambda.doa.audit.ActionExecutionAudit;
import in.handyman.raven.lib.model.DeepSift;
import in.handyman.raven.lib.model.deep.sift.DeepSiftConsumerProcess;
import in.handyman.raven.lib.model.deep.sift.DeepSiftInputTable;
import in.handyman.raven.lib.model.deep.sift.DeepSiftOutputTable;
import in.handyman.raven.core.utils.FileProcessingUtils;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.jdbi.v3.core.Jdbi;
import org.jdbi.v3.core.argument.Arguments;
import org.jdbi.v3.core.argument.NullArgument;
import org.slf4j.Logger;
import org.slf4j.Marker;
import org.slf4j.MarkerFactory;

import java.net.MalformedURLException;
import java.net.URL;
import java.sql.Types;
import java.util.Arrays;
import java.util.Collections;
import java.util.List;
import java.util.Optional;
import java.util.concurrent.LinkedBlockingQueue;
import java.util.stream.Collectors;

/**
 * Auto Generated By Raven
 */
@ActionExecution(actionName = "DeepSift")
public class DeepSiftAction implements IActionExecution {

  public static final String INSERT_COLUMNS = "origin_id,group_id,tenant_id,template_id,process_id, file_path, extracted_text,paper_no,file_name, status,stage,message,is_blank_page, created_on ,root_pipeline_id,template_name,model_name,model_version,batch_id, last_updated_on,request,response,endpoint";
  public static final String INSERT_INTO = "INSERT INTO ";
  public static final String INSERT_INTO_VALUES = "VALUES(?,? ,?,?,? ,?,?,?,?, ?,?,?,?,? ,?, ?,?,?,?,  ?,?,?,?)";
  public static final String READ_BATCH_SIZE = "read.batch.size";
  public static final String TEXT_EXTRACTION_CONSUMER_API_COUNT = "text.extraction.consumer.API.count";
  public static final String WRITE_BATCH_SIZE = "write.batch.size";
  public static final String PAGE_CONTENT_MIN_LENGTH = "page.content.min.length.threshold";
  private final ActionExecutionAudit action;
  public static final String COPRO_FILE_PROCESS_FORMAT = "pipeline.copro.api.process.file.format";

  private final Logger log;

  private final DeepSift DeepSift;
  private final Marker aMarker;
  private final String processBase64;

  public DeepSiftAction(final ActionExecutionAudit action, final Logger log, final Object DeepSift) {
    this.DeepSift = (DeepSift) DeepSift;
    this.action = action;
    this.log = log;
    this.processBase64 = action.getContext().get(COPRO_FILE_PROCESS_FORMAT);

    this.aMarker = MarkerFactory.getMarker(" DeepSift:" + this.DeepSift.getName());
  }

  @Override
  public void execute() throws Exception {
    try {
      final Jdbi jdbi = ResourceAccess.rdbmsJDBIConn(DeepSift.getResourceConn());
      FileProcessingUtils fileProcessingUtils = new FileProcessingUtils(log, aMarker, action);

      jdbi.getConfig(Arguments.class).setUntypedNullArgument(new NullArgument(Types.NULL));
      log.info(aMarker, "Data Extraction Action for {} has been started", DeepSift.getName());

      String outputTableName = DeepSift.getResultTable();
      final String insertQuery = INSERT_INTO + outputTableName + " ( " + INSERT_COLUMNS + " ) " + INSERT_INTO_VALUES;
      final List<URL> urls = Optional.ofNullable(DeepSift.getEndPoint()).map(s -> Arrays.stream(s.split(",")).map(s1 -> {
        try {
          return new URL(s1);
        } catch (MalformedURLException e) {
          log.error("Error in processing the URL ", e);
          throw new HandymanException("Error in processing the URL", e, action);
        }
      }).collect(Collectors.toList())).orElse(Collections.emptyList());

      final CoproProcessor<DeepSiftInputTable, DeepSiftOutputTable> coproProcessor = new CoproProcessor<>(new LinkedBlockingQueue<>(), DeepSiftOutputTable.class, DeepSiftInputTable.class, DeepSift.getResourceConn(), log, new DeepSiftInputTable(), urls, action);

      Integer readBatchSize = Integer.valueOf(action.getContext().get(READ_BATCH_SIZE));
      Integer consumerApiCount = Integer.valueOf(action.getContext().get(TEXT_EXTRACTION_CONSUMER_API_COUNT));
      Integer writeBatchSize = Integer.valueOf(action.getContext().get(WRITE_BATCH_SIZE));
      Integer pageContentMinLength = Integer.valueOf(action.getContext().get(PAGE_CONTENT_MIN_LENGTH));
      DeepSiftConsumerProcess DeepSiftConsumerProcess = new DeepSiftConsumerProcess(log, aMarker, action, pageContentMinLength, fileProcessingUtils, processBase64);

      coproProcessor.startProducer(DeepSift.getQuerySet(), readBatchSize);
      Thread.sleep(1000);
      coproProcessor.startConsumer(insertQuery, consumerApiCount, writeBatchSize, DeepSiftConsumerProcess);
      log.info(aMarker, " Data Extraction Action has been completed {}  ", DeepSift.getName());
    } catch (Exception e) {
      action.getContext().put(DeepSift.getName() + ".isSuccessful", "false");
      log.error(aMarker, "error in execute method in data extraction", e);
      throw new HandymanException("error in execute method in data extraction", e, action);
    }

  }


  @Override
  public boolean executeIf() throws Exception {
    return DeepSift.getCondition();
  }

  @Data
  @AllArgsConstructor
  @Builder
  @NoArgsConstructor
  @JsonIgnoreProperties(ignoreUnknown = true)
  public static class AssetAttributionResponse {
    private String pageContent;
  }

}