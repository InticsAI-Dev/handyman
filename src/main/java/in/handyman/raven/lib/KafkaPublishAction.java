package in.handyman.raven.lib;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import in.handyman.raven.core.utils.ConfigEncryptionUtils;
import in.handyman.raven.exception.HandymanException;
import in.handyman.raven.lambda.access.ResourceAccess;
import in.handyman.raven.lambda.action.ActionExecution;
import in.handyman.raven.lambda.action.IActionExecution;
import in.handyman.raven.lambda.doa.audit.ActionExecutionAudit;
import in.handyman.raven.lib.model.KafkaPublish;
import in.handyman.raven.util.CommonQueryUtil;
import in.handyman.raven.util.EncryptDecrypt;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.clients.producer.RecordMetadata;
import org.apache.kafka.common.errors.RetriableException;
import org.jdbi.v3.core.Jdbi;
import org.jdbi.v3.core.result.ResultIterable;
import org.jdbi.v3.core.statement.Query;
import org.slf4j.Logger;
import org.slf4j.Marker;
import org.slf4j.MarkerFactory;

import javax.crypto.BadPaddingException;
import javax.crypto.IllegalBlockSizeException;
import javax.crypto.NoSuchPaddingException;
import java.security.InvalidKeyException;
import java.security.NoSuchAlgorithmException;
import java.util.*;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.stream.Collectors;

import static in.handyman.raven.core.enums.NetworkHandlerConstants.COPRO_CLIENT_SOCKET_TIMEOUT;

/**
 * Auto Generated By Raven
 */
@ActionExecution(
        actionName = "KafkaPublish"
)
public class KafkaPublishAction implements IActionExecution {
    private final ActionExecutionAudit action;
    private final Logger log;
    private final KafkaPublish kafkaPublish;
    private final Marker aMarker;
    private ObjectMapper objectMapper;

    private static final String SASL_MECHANISM = "sasl.mechanism";
    private static final String SASL_SSL = "SASL_SSL";
    private static final String SCRAM_SHA_256 = "SCRAM-SHA-256";
    private static final String OAUTH_BEARER = "OAUTHBEARER";
    private static final String PLAIN_SASL = "PLAIN";
    private static final String AES_ENCRYPTION = "AES";
    private static final String FAILED_STATUS = "FAILED";

    // Retry configuration

    public KafkaPublishAction(final ActionExecutionAudit action, final Logger log,
                              final Object kafkaPublish) {
        this.kafkaPublish = (KafkaPublish) kafkaPublish;
        this.action = action;
        this.log = log;
        this.aMarker = MarkerFactory.getMarker(" KafkaPublish:" + this.kafkaPublish.getName());
    }

    @Override
    public void execute() throws Exception {
        try {
            objectMapper = new ObjectMapper();
            final Jdbi jdbi = ResourceAccess.rdbmsJDBIConn(kafkaPublish.getResourceConn());
            final List<KafkaPublishQueryInput> kafkaPublishQueryInputs = new ArrayList<>();

            String outputTable = kafkaPublish.getOutputTable();
            jdbi.useTransaction(handle -> handle.execute("create table if not exists " + outputTable +
                    " (id bigserial not null, document_id varchar, checksum varchar, tenant_id int8, origin_id varchar, " +
                    "batch_id varchar, topic_name varchar, endpoint varchar, auth_security_protocol varchar, " +
                    "sasl_mechanism varchar, response varchar, partition varchar, exec_status varchar, " +
                    "retry_count int default 0, created_on timestamp default now(), transaction_id varchar);"));

            jdbi.useTransaction(handle -> {
                final List<String> formattedQuery = CommonQueryUtil.getFormattedQuery(kafkaPublish.getQuerySet());
                AtomicInteger i = new AtomicInteger(0);
                formattedQuery.forEach(sqlToExecute -> {
                    log.info(aMarker, "executing  query {} from index {}", sqlToExecute, i.getAndIncrement());
                    Query query = handle.createQuery(sqlToExecute);
                    ResultIterable<KafkaPublishQueryInput> resultIterable = query.mapToBean(KafkaPublishQueryInput.class);
                    List<KafkaPublishQueryInput> publishQueryInputs = resultIterable.stream().collect(Collectors.toList());
                    kafkaPublishQueryInputs.addAll(publishQueryInputs);
                    log.info(aMarker, "executed query from index {}", i.get());
                });
            });
            int inputQueryObjectSize = kafkaPublishQueryInputs.size();
            log.info(aMarker, "Input Query output has {} objects present", inputQueryObjectSize);
            kafkaPublishQueryInputs.forEach(kafkaPublishQueryInput -> {
                try {
                    doKafkaPublishWithRetry(jdbi, kafkaPublishQueryInput, outputTable);
                } catch (Exception e) {
                    log.error(aMarker, "Failed to publish message after all retry attempts", e);
                    throw new RuntimeException(e);
                }
            });
        } catch (Exception ex) {
            log.error(aMarker, "Error in execute method for kafka publish Action", ex);
            throw new HandymanException("Error in execute method for kafka publish Action", ex, action);
        }
    }

    private void doKafkaPublishWithRetry(Jdbi jdbi, KafkaPublishQueryInput kafkaPublishQueryInput, String outputTable) throws Exception {
        int MAX_RETRY_ATTEMPTS = Integer.parseInt(action.getContext().get("kafka.max.retry.attempts"));
        long INITIAL_RETRY_DELAY_MS = Long.parseLong(action.getContext().get("kafka.initial.retry.delays.ms"));
        final double RETRY_BACKOFF_MULTIPLIER = Double.parseDouble((action.getContext().get("kafka.retry.backoff.multiplier")));

        int attemptCount = 0;
        long retryDelay = INITIAL_RETRY_DELAY_MS;
        Exception lastException = null;

        while (attemptCount < MAX_RETRY_ATTEMPTS) {
            try {
                log.info(aMarker, "Kafka publish attempt {} for document {}", attemptCount + 1, kafkaPublishQueryInput.getDocumentId());
                doKafkaPublish(jdbi, kafkaPublishQueryInput, outputTable, attemptCount);
                log.info(aMarker, "Successfully published message on attempt {}", attemptCount + 1);
                return; // Success, exit retry loop

            } catch (Exception e) {
                lastException = e;
                attemptCount++;

                log.warn(aMarker, "Kafka publish attempt {} failed for document {}. Error: {}",
                        attemptCount, kafkaPublishQueryInput.getDocumentId(), e.getMessage());

                if (attemptCount < MAX_RETRY_ATTEMPTS) {
                    if (isRetriableException(e)) {
                        log.info(aMarker, "Retrying after {} ms (attempt {}/{})",
                                retryDelay, attemptCount, MAX_RETRY_ATTEMPTS);

                        try {
                            Thread.sleep(retryDelay);
                        } catch (InterruptedException ie) {
                            Thread.currentThread().interrupt();
                            throw new HandymanException("Retry interrupted", ie, action);
                        }

                        // Exponential backoff
                        retryDelay = (long) (retryDelay * RETRY_BACKOFF_MULTIPLIER);
                    } else {
                        log.error(aMarker, "Non-retriable exception encountered, stopping retries");
                        break;
                    }
                }
            }
        }

        // All retries exhausted
        String errorMsg = String.format("Failed to publish message after %d attempts for document %s",
                attemptCount, kafkaPublishQueryInput.getDocumentId());
        log.error(aMarker, errorMsg, lastException);

        insertExecutionInfo(jdbi, outputTable,
                kafkaPublishQueryInput.getDocumentId(),
                kafkaPublishQueryInput.getFileChecksum(),
                kafkaPublishQueryInput.getTenantId(),
                kafkaPublishQueryInput.getOriginId(),
                kafkaPublishQueryInput.getBatchId(),
                ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.topic.name")),
                ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.endpoint")),
                ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.auth.security.protocol")),
                ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.sasl.mechanism")),
                "Failed after " + attemptCount + " attempts: " + lastException.getMessage(),
                -1,
                FAILED_STATUS,
                kafkaPublishQueryInput.getTransactionId(),
                attemptCount);

        throw new HandymanException(errorMsg, lastException, action);
    }

    private boolean isRetriableException(Exception e) {
        // Check if exception is retriable
        if (e instanceof RetriableException) {
            return true;
        }

        // Check common retriable errors
        String message = e.getMessage() != null ? e.getMessage().toLowerCase() : "";
        return message.contains("timeout") ||
                message.contains("connection") ||
                message.contains("network") ||
                message.contains("unavailable") ||
                message.contains("broker not available") ||
                message.contains("failed to update metadata") ||
                message.contains("not available") ||
                e instanceof java.net.SocketException ||
                e instanceof java.net.ConnectException ||
                e instanceof java.util.concurrent.TimeoutException ||
                e.getCause() instanceof java.net.SocketTimeoutException;
    }

    private void doKafkaPublish(Jdbi jdbi, KafkaPublishQueryInput kafkaPublishQueryInput, String outputTable, int retryCount) throws Exception {
        log.info(aMarker, "Processing kafka input data for document {}", kafkaPublishQueryInput.getDocumentId());

        String topicName = ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.topic.name"));
        String responseNode = kafkaPublishQueryInput.getJsonData();
        JsonNode productJson = objectMapper.readTree(responseNode);
        String authSecurityProtocol = ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.auth.security.protocol"));
        String saslMechanism = ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.sasl.mechanism"));

        String userName = ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.sasl.username"));
        String password = ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.sasl.password"));
        String endpoint = ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.endpoint"));

        Thread.currentThread().setContextClassLoader(this.getClass().getClassLoader());

        Map<String, Object> kafkaProperties = new HashMap<>();
        kafkaProperties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, endpoint);
        kafkaProperties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");
        kafkaProperties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");

        // Add timeout and retry configurations
        kafkaProperties.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, 30000); // 30 seconds
        kafkaProperties.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, 120000); // 2 minutes
        kafkaProperties.put(ProducerConfig.MAX_BLOCK_MS_CONFIG, 60000); // 1 minute
        kafkaProperties.put(ProducerConfig.RETRIES_CONFIG, 0); // We handle retries manually
        kafkaProperties.put(ProducerConfig.ACKS_CONFIG, "1"); // Wait for leader acknowledgment
        kafkaProperties.put(ProducerConfig.LINGER_MS_CONFIG, 0); // Send immediately

        String messageNode = objectMapper.writeValueAsString(productJson);

        if (Objects.equals(authSecurityProtocol, "NONE")) {
            log.info(aMarker, "Using NONE authentication for Kafka");
            publishToKafka(jdbi, kafkaProperties, topicName, messageNode, outputTable,
                    kafkaPublishQueryInput, authSecurityProtocol, saslMechanism, endpoint, retryCount);
        } else {
            log.info(aMarker, "Using {} authentication for Kafka", authSecurityProtocol);
            if (ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.authentication.sasl.ssl.include")).equals("certs")) {
                setAuthenticationPropertiesWithCerts(authSecurityProtocol, kafkaProperties, saslMechanism, userName, password);
            } else {
                setAuthenticationProperties(authSecurityProtocol, kafkaProperties, saslMechanism, userName, password);
            }
            publishToKafka(jdbi, kafkaProperties, topicName, messageNode, outputTable,
                    kafkaPublishQueryInput, authSecurityProtocol, saslMechanism, endpoint, retryCount);
        }
    }

    private void publishToKafka(Jdbi jdbi, Map<String, Object> kafkaProperties, String topicName,
                                String messageNode, String outputTable, KafkaPublishQueryInput kafkaPublishQueryInput,
                                String authSecurityProtocol, String saslMechanism, String endpoint, int retryCount) throws Exception {

        KafkaProducer<String, String> producer = new KafkaProducer<>(kafkaProperties);;
        try {
            log.info(aMarker, "Creating Kafka producer with config: {}", kafkaProperties.keySet());

            log.info(aMarker, "Created Kafka producer instance successfully");

            ProducerRecord<String, String> producerRecord = new ProducerRecord<>(topicName, messageNode);
            log.info(aMarker, "Created producer record for topic: {}", topicName);

            // Create CompletableFuture to handle async callback
            CompletableFuture<RecordMetadata> future = new CompletableFuture<>();

            // Send message asynchronously
            log.info(aMarker, "Sending message to Kafka...");
            producer.send(producerRecord, (metadata, exception) -> {
                if (exception == null) {
                    log.info(aMarker, "Kafka callback: Message sent successfully");
                    future.complete(metadata);
                } else {
                    log.error(aMarker, "Kafka callback: Error occurred", exception);
                    future.completeExceptionally(exception);
                }
            });

            // Flush to ensure message is sent
            producer.flush();
            log.info(aMarker, "Producer flushed, waiting for result...");

            // Wait for the async result with timeout
            try {
                RecordMetadata metadata = future.get(30, TimeUnit.SECONDS);
                log.info(aMarker, "Successfully sent message to kafka topic: {}, partition: {}, offset: {}",
                        metadata.topic(), metadata.partition(), metadata.offset());

                insertExecutionInfo(jdbi, outputTable,
                        kafkaPublishQueryInput.getDocumentId(),
                        kafkaPublishQueryInput.getFileChecksum(),
                        kafkaPublishQueryInput.getTenantId(),
                        kafkaPublishQueryInput.getOriginId(),
                        kafkaPublishQueryInput.getBatchId(),
                        topicName, endpoint, authSecurityProtocol, saslMechanism,
                        "Successful in sending message to topic",
                        metadata.partition(),
                        "SUCCESS",
                        kafkaPublishQueryInput.getTransactionId(),
                        retryCount);

            } catch (ExecutionException e) {
                log.error(aMarker, "ExecutionException while publishing to Kafka", e);
                Throwable cause = e.getCause();
                if (cause instanceof Exception) {
                    throw (Exception) cause;
                }
                throw new HandymanException("Error publishing message to Kafka", cause, action);
            } catch (java.util.concurrent.TimeoutException e) {
                log.error(aMarker, "Timeout while waiting for Kafka response", e);
                throw new HandymanException("Timeout publishing message to Kafka after 30 seconds", e, action);
            }

        } catch (Exception e) {
            log.error(aMarker, "Exception in publishToKafka method", e);
            throw e;
        } finally {
            if (producer != null) {
                log.info(aMarker, "Closing Kafka producer");
                try {
                    producer.close();
                } catch (Exception e) {
                    log.warn(aMarker, "Error closing Kafka producer", e);
                }
            }
        }

    }

    private void insertExecutionInfo(Jdbi jdbi, String outputTable, String documentId, String checksum, Long tenantId,
                                     String originId, String batchId, String topicName, String endpoint,
                                     String authSecurityProtocol, String saslMechanism, String response,
                                     int partition, String executionStatus, String transactionId, int retryCount) {

        jdbi.useHandle(handle -> handle.createUpdate("INSERT INTO " + outputTable +
                        "(document_id, checksum, tenant_id, origin_id, batch_id, topic_name, endpoint, " +
                        "auth_security_protocol, sasl_mechanism, response, partition, exec_status, transaction_id, retry_count) " +
                        "VALUES(:documentId, :checksum, :tenantId, :originId, :batchId, :topicName, :endpoint, " +
                        ":authSecurityProtocol, :saslMechanism, :response, :partition, :execStatus, :transactionId, :retryCount);")
                .bind("documentId", documentId)
                .bind("checksum", checksum)
                .bind("tenantId", tenantId)
                .bind("originId", originId)
                .bind("batchId", batchId)
                .bind("topicName", topicName)
                .bind("endpoint", endpoint)
                .bind("authSecurityProtocol", authSecurityProtocol)
                .bind("saslMechanism", saslMechanism)
                .bind("response", response)
                .bind("partition", partition)
                .bind("execStatus", executionStatus)
                .bind("transactionId", transactionId)
                .bind("retryCount", retryCount)
                .execute());
    }

    private String doOptionalMessageEncryption(String messageNode, String encryptionType, String encryptionKey) throws NoSuchPaddingException, IllegalBlockSizeException, NoSuchAlgorithmException, BadPaddingException, InvalidKeyException {
        String message = EncryptDecrypt.encrypt(messageNode, encryptionKey, encryptionType);
        log.info("Encrypted message using algorithm {}", encryptionType);
        return message;

    }

    private void setAuthenticationPropertiesWithCerts(String authSecurityProtocol, Map<String, Object> properties, String saslMechanism, String userName, String password) {
        if (authSecurityProtocol.equalsIgnoreCase(SASL_SSL)) {
            properties.put("security.protocol", SASL_SSL);
            if (saslMechanism.equalsIgnoreCase(PLAIN_SASL)) {
                properties.put(SASL_MECHANISM, PLAIN_SASL);

                String jaasConfig = String.format("org.apache.kafka.common.security.plain.PlainLoginModule required username=\"%s\" password=\"%s\";", userName, password);
                properties.put("sasl.jaas.config", jaasConfig);
                properties.put("ssl.truststore.type", ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.truststore.type")));
                properties.put("ssl.keystore.type", ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.keystore.type")));
                properties.put("ssl.truststore.location", ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.truststore.location")));
                properties.put("ssl.truststore.password", ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.truststore.password")));
                properties.put("ssl.keystore.location", ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.keystore.location")));
                properties.put("ssl.keystore.password", ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.keystore.password")));
                properties.put("ssl.key.password", ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.key.password")));
                properties.put("ssl.endpoint.identification.algorithm", ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.endpoint.identification.algorithm")));
                properties.put(ProducerConfig.ACKS_CONFIG, ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.acks")));
                properties.put(ProducerConfig.RETRIES_CONFIG, ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.api.retries")));
                properties.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.request.timeout.ms")));
                properties.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.delivery.timeout.ms")));  // 5 minutes
                properties.put(ProducerConfig.LINGER_MS_CONFIG, ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.linger.ms")));
            } else {
                if (saslMechanism.equalsIgnoreCase(SCRAM_SHA_256)) {
                    properties.put(SASL_MECHANISM, SCRAM_SHA_256);
                    String jaasConfig = String.format("org.apache.kafka.common.security.scram.ScramLoginModule required username=\"%s\" password=\"%s\";", userName, password);
                    properties.put("sasl.jaas.config", jaasConfig);
                } else {
                    if (saslMechanism.equalsIgnoreCase(OAUTH_BEARER)) {
                        properties.put(SASL_MECHANISM, OAUTH_BEARER);
                        properties.put("sasl.login.callback.handler.class", "your.oauth.LoginCallbackHandler");
                    }
                }
            }
        } else if (authSecurityProtocol.equalsIgnoreCase("SASL_PLAINTEXT")) {
            properties.put("security.protocol", "SASL_PLAINTEXT");
            properties.put(SASL_MECHANISM, PLAIN_SASL);
            String jaasConfig = String.format("org.apache.kafka.common.security.plain.PlainLoginModule required username=\"%s\" password=\"%s\";", userName, password);
            properties.put("sasl.jaas.config", jaasConfig);
//            properties.put("sasl.jaas.config",
//                    "org.apache.kafka.common.security.plain.PlainLoginModule required " +
//                            "username=\"" + userName + "\" " +
//                            "password=\"" + password + "\";");

        }
    }


    private static void setAuthenticationProperties(String authSecurityProtocol, Map<String, Object> properties, String saslMechanism, String userName, String password) {
        if (authSecurityProtocol.equalsIgnoreCase(SASL_SSL)) {
            properties.put("security.protocol", SASL_SSL);
            if (saslMechanism.equalsIgnoreCase(PLAIN_SASL)) {
                properties.put(SASL_MECHANISM, PLAIN_SASL);
                String jaasConfig = String.format("org.apache.kafka.common.security.plain.PlainLoginModule required username=\"%s\" password=\"%s\";", userName, password);
                properties.put("sasl.jaas.config", jaasConfig);
            } else {
                if (saslMechanism.equalsIgnoreCase(SCRAM_SHA_256)) {
                    properties.put(SASL_MECHANISM, SCRAM_SHA_256);
                    String jaasConfig = String.format("org.apache.kafka.common.security.scram.ScramLoginModule required username=\"%s\" password=\"%s\";", userName, password);
                    properties.put("sasl.jaas.config", jaasConfig);
                } else {
                    if (saslMechanism.equalsIgnoreCase(OAUTH_BEARER)) {
                        properties.put(SASL_MECHANISM, OAUTH_BEARER);
                        properties.put("sasl.login.callback.handler.class", "your.oauth.LoginCallbackHandler");
                    }
                }
            }
        } else if (authSecurityProtocol.equalsIgnoreCase("SASL_PLAINTEXT")) {
            properties.put("security.protocol", "SASL_PLAINTEXT");
            properties.put(SASL_MECHANISM, PLAIN_SASL);
            String jaasConfig = String.format("org.apache.kafka.common.security.plain.PlainLoginModule required username=\"%s\" password=\"%s\";", userName, password);
            properties.put("sasl.jaas.config", jaasConfig);
//            properties.put("sasl.jaas.config",
//                    "org.apache.kafka.common.security.plain.PlainLoginModule required " +
//                            "username=\"" + userName + "\" " +
//                            "password=\"" + password + "\";");

        }
    }

    @Data
    @AllArgsConstructor
    @NoArgsConstructor
    @Builder
    public static class KafkaPublishQueryInput {
        private String endpoint;
        private String topicName;
        private String authSecurityProtocol;
        private String saslMechanism;
        private String userName;
        private String password;
        private String encryptionType;
        private String encryptionKey;
        private String jsonData;
        private String documentId;
        private String fileChecksum;
        private String originId;
        private String batchId;
        private Long tenantId;
        private String transactionId;
    }

    @Override
    public boolean executeIf() throws Exception {
        return kafkaPublish.getCondition();
    }
}
