package in.handyman.raven.lib;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import in.handyman.raven.core.utils.ConfigEncryptionUtils;
import in.handyman.raven.exception.HandymanException;
import in.handyman.raven.lambda.access.ResourceAccess;
import in.handyman.raven.lambda.action.ActionExecution;
import in.handyman.raven.lambda.action.IActionExecution;
import in.handyman.raven.lambda.doa.audit.ActionExecutionAudit;
import in.handyman.raven.lib.model.KafkaPublish;
import in.handyman.raven.util.CommonQueryUtil;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.jdbi.v3.core.Jdbi;
import org.jdbi.v3.core.result.ResultIterable;
import org.jdbi.v3.core.statement.PreparedBatch;
import org.jdbi.v3.core.statement.Query;
import org.slf4j.Logger;
import org.slf4j.Marker;
import org.slf4j.MarkerFactory;

import java.time.LocalDateTime;
import java.util.*;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.stream.Collectors;

/**
 * Auto Generated By Raven
 */
@ActionExecution(
        actionName = "KafkaPublish"
)
public class KafkaPublishAction implements IActionExecution {
    private final ActionExecutionAudit action;

    private final Logger log;

    private final KafkaPublish kafkaPublish;

    private final Marker aMarker;
    private ObjectMapper objectMapper;
    private static final String SASL_MECHANISM = "sasl.mechanism";
    private static final String SASL_SSL = "SASL_SSL";
    private static final String SCRAM_SHA_256 = "SCRAM-SHA-256";
    private static final String OAUTH_BEARER = "OAUTHBEARER";
    private static final String PLAIN_SASL = "PLAIN";
    private static final String AES_ENCRYPTION = "AES";
    private static final String FAILED_STATUS = "FAILED";
    private static final String ENABLE_RETRY_KEY = "kafka.enable.retry";

    public KafkaPublishAction(final ActionExecutionAudit action, final Logger log,
                              final Object kafkaPublish) {
        this.kafkaPublish = (KafkaPublish) kafkaPublish;
        this.action = action;
        this.log = log;
        this.aMarker = MarkerFactory.getMarker(" KafkaPublish:" + this.kafkaPublish.getName());
    }


    @Override
    public void execute() throws Exception {

        try {
            objectMapper = new ObjectMapper();
            final Jdbi jdbi = ResourceAccess.rdbmsJDBIConn(kafkaPublish.getResourceConn());
            final List<KafkaPublishQueryInput> kafkaPublishQueryInputs = new ArrayList<>();

            final List<KafkaProductionResponseAudit> auditRecords = new ArrayList<>();

            String outputTable = kafkaPublish.getOutputTable();

            jdbi.useTransaction(handle -> handle.execute("create table if not exists " + outputTable +
                    " (id bigserial not null, document_id varchar, checksum varchar, tenant_id int8, " +
                    "origin_id varchar, batch_id varchar, topic_name varchar, endpoint varchar, " +
                    "auth_security_protocol varchar, sasl_mechanism varchar, response varchar, " +
                    "partition  int8, exec_status varchar, transaction_id varchar, root_pipeline_id  int8, " +
                    "created_user_id  int8, last_updated_user_id  int8, created_on timestamp default now(), " +
                    "last_updated_on timestamp, status varchar, version int, retry_count int default 0);"));

            jdbi.useTransaction(handle -> {
                final List<String> formattedQuery = CommonQueryUtil.getFormattedQuery(kafkaPublish.getQuerySet());
                AtomicInteger i = new AtomicInteger(0);
                formattedQuery.forEach(sqlToExecute -> {
                    log.info(aMarker, "executing  query {} from index {}", sqlToExecute, i.getAndIncrement());
                    Query query = handle.createQuery(sqlToExecute);
                    ResultIterable<KafkaPublishQueryInput> resultIterable = query.mapToBean(KafkaPublishQueryInput.class);
                    List<KafkaPublishQueryInput> publishQueryInputs = resultIterable.stream().collect(Collectors.toList());
                    kafkaPublishQueryInputs.addAll(publishQueryInputs);
                    log.info(aMarker, "executed query from index {}", i.get());
                });
            });
            int inputQueryObjectSize = kafkaPublishQueryInputs.size();
            log.info(aMarker, "Input Query output has {} objects present", inputQueryObjectSize);
            kafkaPublishQueryInputs.forEach(kafkaPublishQueryInput -> {
                try {
                    KafkaProductionResponseAudit audit = doKafkaPublishWithRetry(jdbi, kafkaPublishQueryInput, outputTable);
                    if (audit != null) {
                        auditRecords.add(audit);
                    }
                } catch (Exception e) {
                    log.error(aMarker, "Error processing kafka publish query input", e);
                    HandymanException handymanException = new HandymanException(e);
                    HandymanException.insertException("Error processing kafka publish query input", handymanException, action);
                    throw new RuntimeException(e);
                }
            });

            if (!auditRecords.isEmpty()) {
                insertExecutionInfoBatch(jdbi, outputTable, auditRecords);
                log.info(aMarker, "Batch inserted {} audit records", auditRecords.size());
            }

        } catch (Exception ex) {
            log.error(aMarker, "Error in execute method for kafka publish Action", ex);
            HandymanException handymanException = new HandymanException(ex);
            HandymanException.insertException("Error in execute method for kafka publish Action", handymanException, action);
            throw new HandymanException("Error in execute method for kafka publish Action", ex, action);
        }
    }

    private KafkaProductionResponseAudit doKafkaPublishWithRetry(Jdbi jdbi, KafkaPublishQueryInput kafkaPublishQueryInput,
                                                                 String outputTable) throws Exception {
        String topicName = ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.topic.name"));
        String authSecurityProtocol = ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.auth.security.protocol"));
        String saslMechanism = ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.sasl.mechanism"));
        String endpoint = ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.endpoint"));

        KafkaProductionResponseAudit audit = getKafkaPublishOutputAudit(kafkaPublishQueryInput, topicName,
                endpoint, authSecurityProtocol, saslMechanism);

        // Check if retry is enabled
        boolean enableRetry = Boolean.parseBoolean(action.getContext().getOrDefault(ENABLE_RETRY_KEY, "true"));

        if (!enableRetry) {
            log.info(aMarker, "Retry is disabled. Attempting Kafka publish once for document {}",
                    kafkaPublishQueryInput.getDocumentId());
            try {
                doKafkaPublish(kafkaPublishQueryInput, 0, audit);
                log.info(aMarker, "Successfully published message without retry");
                return audit;
            } catch (Exception e) {
                log.error(aMarker, "Failed to publish message (retry disabled) for document {}",
                        kafkaPublishQueryInput.getDocumentId(), e);

                updateKafkaPublishAudit(-1, FAILED_STATUS, audit,
                        "Failed (retry disabled): " + e.getMessage());
                audit.setRetryCount(0);

                throw new HandymanException("Failed to publish message (retry disabled)", e, action);
            }
        }

        // Manual retry logic with exponential backoff
        int MAX_RETRY_ATTEMPTS = Integer.parseInt(action.getContext().getOrDefault("kafka.max.retry.attempts", "3"));
        long INITIAL_RETRY_DELAY_MS = Long.parseLong(action.getContext().getOrDefault("kafka.initial.retry.delays.ms", "1000"));
        double RETRY_BACKOFF_MULTIPLIER = Double.parseDouble(action.getContext().getOrDefault("kafka.retry.backoff.multiplier", "2.0"));

        log.info(aMarker, "Retry is enabled. Max attempts: {}", MAX_RETRY_ATTEMPTS);

        int attemptCount = 0;
        long retryDelay = INITIAL_RETRY_DELAY_MS;
        Exception lastException = null;

        while (attemptCount < MAX_RETRY_ATTEMPTS) {
            try {
                log.info(aMarker, "Kafka publish attempt {} for document {}",
                        attemptCount + 1, kafkaPublishQueryInput.getDocumentId());

                doKafkaPublish(kafkaPublishQueryInput, attemptCount, audit);

                log.info(aMarker, "Successfully published message on attempt {}", attemptCount + 1);
                audit.setRetryCount(attemptCount);
                return audit;

            } catch (Exception e) {
                lastException = e;
                attemptCount++;

                log.warn(aMarker, "Kafka publish attempt {} failed for document {}. Error: {}",
                        attemptCount, kafkaPublishQueryInput.getDocumentId(), e.getMessage());

                if (attemptCount < MAX_RETRY_ATTEMPTS) {
                    log.info(aMarker, "Retrying after {} ms (attempt {}/{})",
                            retryDelay, attemptCount, MAX_RETRY_ATTEMPTS);

                    try {
                        Thread.sleep(retryDelay);
                    } catch (InterruptedException ie) {
                        Thread.currentThread().interrupt();
                        throw new HandymanException("Retry interrupted", ie, action);
                    }

                    retryDelay = (long) (retryDelay * RETRY_BACKOFF_MULTIPLIER);
                }
            }
        }

        // All retries exhausted
        String errorMsg = String.format("Failed to publish message after %d attempts for document %s",
                attemptCount, kafkaPublishQueryInput.getDocumentId());
        log.error(aMarker, errorMsg, lastException);

        updateKafkaPublishAudit(-1, FAILED_STATUS, audit,
                "Failed after " + attemptCount + " attempts: " + lastException.getMessage());
        audit.setRetryCount(attemptCount);

        throw new HandymanException(errorMsg, lastException, action);
    }

    private void doKafkaPublish(KafkaPublishQueryInput kafkaPublishQueryInput, Integer retryCount, KafkaProductionResponseAudit audit) throws JsonProcessingException {
        log.info("processing the kafka input data ");

        String topicName = ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.topic.name"));
        String responseNode = kafkaPublishQueryInput.getJsonData();
        JsonNode productJson = objectMapper.readTree(responseNode);
        String authSecurityProtocol = ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.auth.security.protocol"));
        String saslMechanism = ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.sasl.mechanism"));
        String userName = ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.sasl.username"));
        String password = ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.sasl.password"));
        String endpoint = ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.endpoint"));

        Thread.currentThread().setContextClassLoader(this.getClass().getClassLoader());

        Map<String, Object> kafkaProperties = new HashMap<>();
        kafkaProperties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, endpoint);
        kafkaProperties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");
        kafkaProperties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");


        if (Objects.equals(authSecurityProtocol, "NONE")) {
            try {


                String messageNode = "";
                try {
                    messageNode = objectMapper.writeValueAsString(productJson);
                } catch (Exception e) {
                    log.error(aMarker, "Error in converting json data to kafka topic message {}", authSecurityProtocol, e);
                    HandymanException handymanException = new HandymanException(e);
                    HandymanException.insertException("Error in converting json data to kafka topic message", handymanException, action);
                }
                KafkaProducer<String, String> producer = new KafkaProducer<>(kafkaProperties);
                log.info("Creating the kafka producer instance");

                ProducerRecord<String, String> producerRecord = new ProducerRecord<>(topicName, messageNode);

                try {
                    producer.send(producerRecord, (metadata, exception) -> {
                        if (exception == null) {
                            log.info("Successful in sending the message to kafka topic");
                            int partition = metadata.partition();
                            log.info("Topic: {}, Partition: {}", metadata.topic(), partition);
//                            insertExecutionInfo(jdbi, outputTable, documentId, fileChecksum, tenantId, originId, batchId, topicName, endpoint, authSecurityProtocol, saslMechanism, "Successful in sending message to topic", partition, "SUCCESS", transactionId);
                        } else {
                            log.error(aMarker, "Error in publishing message to kafka topic", exception);
                            HandymanException handymanException = new HandymanException(exception);
                            HandymanException.insertException("Error in publishing message to kafka topic", handymanException, action);
                        }
                    });
                } finally {
                    log.info("Closing Producer");
                    producer.close();
                }
            } catch (HandymanException e) {
                log.info("Error in posting kafka topic message");
                HandymanException handymanException = new HandymanException(e);
                HandymanException.insertException("Error in posting kafka topic message", handymanException, action);
            }
        } else {

            try {
                log.info("Processing kafka with authentication");

                if (ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.authentication.sasl.ssl.include")).equals("certs")) {
                    setAuthenticationPropertiesWithCerts(authSecurityProtocol, kafkaProperties, saslMechanism, userName, password);

                } else {
                    setAuthenticationProperties(authSecurityProtocol, kafkaProperties, saslMechanism, userName, password);

                }
                KafkaProducer<String, String> producer = new KafkaProducer<>(kafkaProperties);
                log.info("creating the kafka instance");

//            if (encryptionType.equalsIgnoreCase(AES_ENCRYPTION)) {
//                responseNode = doOptionalMessageEncryption(responseNode, encryptionType, encryptionKey);
//            }

                String messageNode = "";
                try {
                    messageNode = objectMapper.writeValueAsString(productJson);
                } catch (Exception e) {
                    log.error(aMarker, "Error in converting json data to kafka topic message", e);
                    HandymanException handymanException = new HandymanException(e);
                    HandymanException.insertException("Error in converting json data to kafka topic message", handymanException, action);
                }


                ProducerRecord<String, String> producerRecord = new ProducerRecord<>(topicName, messageNode);
                log.info("sending the topic and output json");

                try {
                    producer.send(producerRecord, (metadata, exception) -> {
                        if (exception == null) {
                            log.info("✅ Message sent successfully after {} retries", retryCount);
                            int partition = metadata.partition();
                            updateKafkaPublishAudit(partition, "SUCCESS", audit,
                                    String.format("Success after %d retries", retryCount));
                        } else {
                            log.error(aMarker, "❌ Failed after {} retries: {}", retryCount, exception.getMessage());
                            updateKafkaPublishAudit(-1, FAILED_STATUS, audit,
                                    String.format("Failed after %d retries: %s", retryCount, exception.getMessage()));
                            HandymanException handymanException = new HandymanException(exception);
                            HandymanException.insertException("Error in publishing message to kafka topic", handymanException, action);
                        }
                    });
                } finally {
                    producer.close();
                }
            } catch (HandymanException e) {
                HandymanException handymanException = new HandymanException(e);
                HandymanException.insertException("Error in posting kafka topic message", handymanException, action);
            }
        }

    }

    private void setAuthenticationPropertiesWithCerts(String authSecurityProtocol, Map<String, Object> properties,
                                                      String saslMechanism, String userName, String password) {
        if (authSecurityProtocol.equalsIgnoreCase(SASL_SSL)) {
            properties.put("security.protocol", SASL_SSL);
            if (saslMechanism.equalsIgnoreCase(PLAIN_SASL)) {
                properties.put(SASL_MECHANISM, PLAIN_SASL);

                String jaasConfig = String.format("org.apache.kafka.common.security.plain.PlainLoginModule required username=\"%s\" password=\"%s\";", userName, password);
                properties.put("sasl.jaas.config", jaasConfig);
                properties.put("ssl.truststore.type", ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.truststore.type")));
                properties.put("ssl.keystore.type", ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.keystore.type")));
                properties.put("ssl.truststore.location", ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.truststore.location")));
                properties.put("ssl.truststore.password", ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.truststore.password")));
                properties.put("ssl.keystore.location", ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.keystore.location")));
                properties.put("ssl.keystore.password", ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.keystore.password")));
                properties.put("ssl.key.password", ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.key.password")));
                properties.put("ssl.endpoint.identification.algorithm", ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.endpoint.identification.algorithm")));
                properties.put(ProducerConfig.ACKS_CONFIG, ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.acks")));
                properties.put(ProducerConfig.RETRIES_CONFIG, ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.api.retries")));
                properties.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.request.timeout.ms")));
                properties.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.delivery.timeout.ms")));  // 5 minutes
                properties.put(ProducerConfig.LINGER_MS_CONFIG, ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.linger.ms")));
            } else {
                if (saslMechanism.equalsIgnoreCase(SCRAM_SHA_256)) {
                    properties.put(SASL_MECHANISM, SCRAM_SHA_256);
                    String jaasConfig = String.format("org.apache.kafka.common.security.scram.ScramLoginModule required username=\"%s\" password=\"%s\";", userName, password);
                    properties.put("sasl.jaas.config", jaasConfig);
                } else {
                    if (saslMechanism.equalsIgnoreCase(OAUTH_BEARER)) {
                        properties.put(SASL_MECHANISM, OAUTH_BEARER);
                        properties.put("sasl.login.callback.handler.class", "your.oauth.LoginCallbackHandler");
                    }
                }
            }
        } else if (authSecurityProtocol.equalsIgnoreCase("SASL_PLAINTEXT")) {
            properties.put("security.protocol", "SASL_PLAINTEXT");
            properties.put(SASL_MECHANISM, PLAIN_SASL);
            String jaasConfig = String.format("org.apache.kafka.common.security.plain.PlainLoginModule required username=\"%s\" password=\"%s\";", userName, password);
            properties.put("sasl.jaas.config", jaasConfig);
//            properties.put("sasl.jaas.config",
//                    "org.apache.kafka.common.security.plain.PlainLoginModule required " +
//                            "username=\"" + userName + "\" " +
//                            "password=\"" + password + "\";");

        }
    }


    private static void setAuthenticationProperties(String authSecurityProtocol, Map<String, Object> properties, String saslMechanism, String userName, String password) {
        if (authSecurityProtocol.equalsIgnoreCase(SASL_SSL)) {
            properties.put("security.protocol", SASL_SSL);
            if (saslMechanism.equalsIgnoreCase(PLAIN_SASL)) {
                properties.put(SASL_MECHANISM, PLAIN_SASL);
                String jaasConfig = String.format("org.apache.kafka.common.security.plain.PlainLoginModule required username=\"%s\" password=\"%s\";", userName, password);
                properties.put("sasl.jaas.config", jaasConfig);
            } else {
                if (saslMechanism.equalsIgnoreCase(SCRAM_SHA_256)) {
                    properties.put(SASL_MECHANISM, SCRAM_SHA_256);
                    String jaasConfig = String.format("org.apache.kafka.common.security.scram.ScramLoginModule required username=\"%s\" password=\"%s\";", userName, password);
                    properties.put("sasl.jaas.config", jaasConfig);
                } else {
                    if (saslMechanism.equalsIgnoreCase(OAUTH_BEARER)) {
                        properties.put(SASL_MECHANISM, OAUTH_BEARER);
                        properties.put("sasl.login.callback.handler.class", "your.oauth.LoginCallbackHandler");
                    }
                }
            }
        } else if (authSecurityProtocol.equalsIgnoreCase("SASL_PLAINTEXT")) {
            properties.put("security.protocol", "SASL_PLAINTEXT");
            properties.put(SASL_MECHANISM, PLAIN_SASL);
            String jaasConfig = String.format("org.apache.kafka.common.security.plain.PlainLoginModule required username=\"%s\" password=\"%s\";", userName, password);
            properties.put("sasl.jaas.config", jaasConfig);
        }
    }

    private KafkaProductionResponseAudit getKafkaPublishOutputAudit(KafkaPublishQueryInput kafkaPublishQueryInput,
                                                                    String topicName, String endpoint,
                                                                    String authSecurityProtocol, String saslMechanism) {
        KafkaProductionResponseAudit audit = new KafkaProductionResponseAudit();
        audit.setDocumentId(kafkaPublishQueryInput.getDocumentId());
        audit.setChecksum(kafkaPublishQueryInput.getFileChecksum());
        audit.setTenantId(kafkaPublishQueryInput.getTenantId());
        audit.setOriginId(kafkaPublishQueryInput.getOriginId());
        audit.setTopicName(topicName);
        audit.setEndpoint(endpoint);
        audit.setAuthSecurityProtocol(authSecurityProtocol);
        audit.setSaslMechanism(saslMechanism);
        audit.setBatchId(kafkaPublishQueryInput.getBatchId());
        audit.setTransactionId(kafkaPublishQueryInput.getTransactionId());
        audit.setCreatedUserId(kafkaPublishQueryInput.getTenantId());
        audit.setLastUpdatedUserId(kafkaPublishQueryInput.getTenantId());
        audit.setCreatedOn(LocalDateTime.now());
        audit.setVersion(1);
        audit.setRetryCount(0);
        return audit;
    }

    private void updateKafkaPublishAudit(int partition, String executionStatus,
                                         KafkaProductionResponseAudit kafkaPublishOutput,
                                         String response) {
        kafkaPublishOutput.setPartition(partition);
        kafkaPublishOutput.setExecStatus(executionStatus);
        kafkaPublishOutput.setLastUpdatedOn(LocalDateTime.now());
        kafkaPublishOutput.setResponse(response);
        kafkaPublishOutput.setStatus("ACTIVE");
    }

    private void insertExecutionInfoBatch(Jdbi jdbi, String outputTable,
                                          List<KafkaProductionResponseAudit> kafkaProductionResponseAudits) {
        if (kafkaProductionResponseAudits == null || kafkaProductionResponseAudits.isEmpty()) {
            log.warn("No KafkaProductionResponseAudit records to insert into table");
            return;
        }

        String sql = "INSERT INTO " + outputTable + " (" +
                "document_id, checksum, tenant_id, origin_id, batch_id, topic_name, endpoint, " +
                "auth_security_protocol, sasl_mechanism, response, partition, exec_status, " +
                "transaction_id, root_pipeline_id, created_user_id, last_updated_user_id, " +
                "created_on, last_updated_on, status, version, retry_count" +
                ") VALUES (" +
                ":documentId, :checksum, :tenantId, :originId, :batchId, :topicName, :endpoint, " +
                ":authSecurityProtocol, :saslMechanism, :response, :partition, :execStatus, " +
                ":transactionId, :rootPipelineId, :createdUserId, :lastUpdatedUserId, " +
                ":createdOn, :lastUpdatedOn, :status, :version, :retryCount" +
                ")";

        jdbi.useHandle(handle -> {
            PreparedBatch batch = handle.prepareBatch(sql);

            for (KafkaProductionResponseAudit record : kafkaProductionResponseAudits) {
                batch.bind("documentId", record.getDocumentId())
                        .bind("checksum", record.getChecksum())
                        .bind("tenantId", record.getTenantId())
                        .bind("originId", record.getOriginId())
                        .bind("batchId", record.getBatchId())
                        .bind("topicName", record.getTopicName())
                        .bind("endpoint", record.getEndpoint())
                        .bind("authSecurityProtocol", record.getAuthSecurityProtocol())
                        .bind("saslMechanism", record.getSaslMechanism())
                        .bind("response", record.getResponse())
                        .bind("partition", record.getPartition())
                        .bind("execStatus", record.getExecStatus())
                        .bind("transactionId", record.getTransactionId())
                        .bind("rootPipelineId", record.getRootPipelineId())
                        .bind("createdUserId", record.getCreatedUserId())
                        .bind("lastUpdatedUserId", record.getLastUpdatedUserId())
                        .bind("createdOn", record.getCreatedOn())
                        .bind("lastUpdatedOn", record.getLastUpdatedOn())
                        .bind("status", record.getStatus())
                        .bind("version", record.getVersion())
                        .bind("retryCount", record.getRetryCount())
                        .add();
            }

            int[] results = batch.execute();
            log.info("✅ Batch insert completed into [{}]. Total Records: {}, Successful Inserts: {}",
                    outputTable, kafkaProductionResponseAudits.size(), results.length);
        });
    }

    @Data
    @AllArgsConstructor
    @NoArgsConstructor
    @Builder
    public static class KafkaProductionResponseAudit {
        private String documentId;
        private String checksum;
        private Long tenantId;
        private String originId;
        private String batchId;
        private String topicName;
        private String endpoint;
        private String authSecurityProtocol;
        private String saslMechanism;
        private String response;
        private Integer partition;
        private String execStatus;
        private String transactionId;
        private Long rootPipelineId;
        private Long createdUserId;
        private Long lastUpdatedUserId;
        private LocalDateTime createdOn;
        private LocalDateTime lastUpdatedOn;
        private String status;
        private Integer version;
        private Integer retryCount;
    }

    @Data
    @AllArgsConstructor
    @NoArgsConstructor
    @Builder
    public static class KafkaPublishQueryInput {
        private String endpoint;
        private String topicName;
        private String authSecurityProtocol;
        private String saslMechanism;
        private String userName;
        private String password;
        private String encryptionType;
        private String encryptionKey;
        private String jsonData;
        private String documentId;
        private String fileChecksum;
        private String originId;
        private String batchId;
        private Long tenantId;
        private String transactionId;
    }

    @Override
    public boolean executeIf() throws Exception {
        return kafkaPublish.getCondition();
    }
}