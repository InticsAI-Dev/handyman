package in.handyman.raven.lib;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import in.handyman.raven.core.utils.ConfigEncryptionUtils;
import in.handyman.raven.exception.HandymanException;
import in.handyman.raven.lambda.access.ResourceAccess;
import in.handyman.raven.lambda.action.ActionExecution;
import in.handyman.raven.lambda.action.IActionExecution;
import in.handyman.raven.lambda.doa.audit.ActionExecutionAudit;
import in.handyman.raven.lib.model.KafkaPublish;
import in.handyman.raven.util.CommonQueryUtil;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.jdbi.v3.core.Jdbi;
import org.jdbi.v3.core.result.ResultIterable;
import org.jdbi.v3.core.statement.PreparedBatch;
import org.jdbi.v3.core.statement.Query;
import org.slf4j.Logger;
import org.slf4j.Marker;
import org.slf4j.MarkerFactory;

import java.time.LocalDateTime;
import java.util.*;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.atomic.AtomicReference;
import java.util.stream.Collectors;

import static in.handyman.raven.lib.kafka.KafkaProps.OAUTH_BEARER;

/**
 * Auto Generated By Raven
 */
@ActionExecution(
        actionName = "KafkaPublish"
)
public class KafkaPublishAction implements IActionExecution {
    private final ActionExecutionAudit action;

    private final Logger log;

    private final KafkaPublish kafkaPublish;

    private final Marker aMarker;
    private ObjectMapper objectMapper;
    private static final String SASL_MECHANISM = "sasl.mechanism";
    private static final String SASL_SSL = "SASL_SSL";
    private static final String SCRAM_SHA_256 = "SCRAM-SHA-256";
    private static final String PLAIN_SASL = "PLAIN";
    private static final String FAILED_STATUS = "FAILED";
    private static final String KAFKA_ENABLE_MANUAL_RETRY_KEY = "kafka.enable.manual.retry";
    private static final String KAFKA_ENABLE_INBUILT_RETRY_KEY = "kafka.enable.inbuild.retry";

    public KafkaPublishAction(final ActionExecutionAudit action, final Logger log,
                              final Object kafkaPublish) {
        this.kafkaPublish = (KafkaPublish) kafkaPublish;
        this.action = action;
        this.log = log;
        this.aMarker = MarkerFactory.getMarker(" KafkaPublish:" + this.kafkaPublish.getName());
    }


    @Override
    public void execute() throws Exception {

        try {
            objectMapper = new ObjectMapper();
            final Jdbi jdbi = ResourceAccess.rdbmsJDBIConn(kafkaPublish.getResourceConn());
            final List<KafkaPublishQueryInput> kafkaPublishQueryInputs = new ArrayList<>();

            String outputTable = kafkaPublish.getOutputTable();

            jdbi.useTransaction(handle -> handle.execute("create table if not exists " + outputTable +
                    " (id bigserial not null, document_id varchar, checksum varchar, tenant_id int8, " +
                    "origin_id varchar, batch_id varchar, topic_name varchar, endpoint varchar, " +
                    "auth_security_protocol varchar, sasl_mechanism varchar, response varchar, " +
                    "partition  int8, exec_status varchar, transaction_id varchar, root_pipeline_id  int8, " +
                    "created_user_id  int8, last_updated_user_id  int8, created_on timestamp default now(), " +
                    "last_updated_on timestamp, status varchar, version int, retry_count int default 0);"));

            jdbi.useTransaction(handle -> {
                final List<String> formattedQuery = CommonQueryUtil.getFormattedQuery(kafkaPublish.getQuerySet());
                AtomicInteger i = new AtomicInteger(0);
                formattedQuery.forEach(sqlToExecute -> {
                    log.info(aMarker, "executing  query {} from index {}", sqlToExecute, i.getAndIncrement());
                    Query query = handle.createQuery(sqlToExecute);
                    ResultIterable<KafkaPublishQueryInput> resultIterable = query.mapToBean(KafkaPublishQueryInput.class);
                    List<KafkaPublishQueryInput> publishQueryInputs = resultIterable.stream().collect(Collectors.toList());
                    kafkaPublishQueryInputs.addAll(publishQueryInputs);
                    log.info(aMarker, "executed query from index {}", i.get());
                });
            });

            int inputQueryObjectSize = kafkaPublishQueryInputs.size();
            log.info(aMarker, "Input Query output has {} objects present", inputQueryObjectSize);

            final List<KafkaProductionResponseAudit> allAuditRecords = new ArrayList<>();

            for (KafkaPublishQueryInput input : kafkaPublishQueryInputs) {
                try {

                            doKafkaPublishWithRetry(jdbi, input, outputTable, allAuditRecords);
                } catch (Exception e) {

                    log.error(aMarker, "Error processing kafka publish query input {}", input.getDocumentId(), e);
                    HandymanException handymanException = new HandymanException(e);
                    HandymanException.insertException("Error processing kafka publish query input", handymanException, action);
                }
            }

            if (!allAuditRecords.isEmpty()) {
                insertExecutionInfoBatch(jdbi, outputTable, allAuditRecords);
                log.info(aMarker, "=== BATCH INSERT COMPLETED === {} total audit records", allAuditRecords.size());
            } else {
                log.info(aMarker, "No audit records to insert");
            }

        } catch (Exception ex) {
            log.error(aMarker, "Error in execute method for kafka publish Action", ex);
            HandymanException handymanException = new HandymanException(ex);
            HandymanException.insertException("Error in execute method for kafka publish Action", handymanException, action);
            throw new HandymanException("Error in execute method for kafka publish Action", ex, action);
        }
    }

    private List<KafkaProductionResponseAudit> doKafkaPublishWithRetry(
            Jdbi jdbi, KafkaPublishQueryInput kafkaPublishQueryInput,
            String outputTable, List<KafkaProductionResponseAudit> allAuditRecords) throws Exception {

        String topicName = ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.topic.name"));
        String authSecurityProtocol = ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.auth.security.protocol"));
        String saslMechanism = ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.sasl.mechanism"));
        String endpoint = ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.endpoint"));

        boolean enableRetry = Boolean.parseBoolean(action.getContext().getOrDefault(KAFKA_ENABLE_MANUAL_RETRY_KEY, "true"));
        int maxAttempts = enableRetry ? Integer.parseInt(action.getContext().getOrDefault("kafka.max.retry.attempts", "3")) : 1;
        long initialDelay = Long.parseLong(action.getContext().getOrDefault("kafka.initial.retry.delays.ms", "1000"));
        double backoffMultiplier = Double.parseDouble(action.getContext().getOrDefault("kafka.retry.backoff.multiplier", "2.0"));

        List<KafkaProductionResponseAudit> localAudits = new ArrayList<>();

        Exception lastException = null;
        long retryDelay = initialDelay;

        for (int attempt = 0; attempt < maxAttempts; attempt++) {
            KafkaProductionResponseAudit audit = getKafkaPublishOutputAudit(kafkaPublishQueryInput, topicName,
                    endpoint, authSecurityProtocol, saslMechanism);
            audit.setRootPipelineId(action.getRootPipelineId());
            audit.setRetryCount(attempt);

            try {
                log.info(aMarker, "Kafka publish attempt {}/{} for document: {}",
                        attempt + 1, maxAttempts, kafkaPublishQueryInput.getDocumentId());

                doKafkaPublish(kafkaPublishQueryInput, attempt, audit);

                log.info(aMarker, "Published successfully on attempt {}/{} for document: {}",
                        attempt + 1, maxAttempts, kafkaPublishQueryInput.getDocumentId());

                localAudits.add(audit);
                break;

            } catch (Exception e) {
                lastException = e;
                String errorMsg = String.format("Attempt %d/%d failed: %s",
                        attempt + 1, maxAttempts, e.getMessage());

                log.warn(aMarker, "{} for document: {}", errorMsg, kafkaPublishQueryInput.getDocumentId());
                audit.setPartition(-1);
                audit.setExecStatus(FAILED_STATUS);
                audit.setResponse(errorMsg);
                audit.setLastUpdatedOn(LocalDateTime.now());
                audit.setStatus("ACTIVE");

                localAudits.add(audit);

                if (attempt < maxAttempts - 1) {
                    try {
                        log.info(aMarker, "Waiting {}ms before retry...", retryDelay);
                        Thread.sleep(retryDelay);
                        retryDelay = (long) (retryDelay * backoffMultiplier);
                    } catch (InterruptedException ie) {
                        Thread.currentThread().interrupt();
                        lastException = ie;
                        break;
                    }
                }
            }
        }

        allAuditRecords.addAll(localAudits);

        if (lastException != null) {
            String finalErrorMsg = String.format("Failed after %d attempts: %s",
                    maxAttempts, lastException.getMessage());
            throw new HandymanException(finalErrorMsg, lastException, action);
        }

        return localAudits;
    }

    private void doKafkaPublish(KafkaPublishQueryInput kafkaPublishQueryInput, Integer retryCount,
                                KafkaProductionResponseAudit audit) throws Exception {

        String topicName = ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.topic.name"));
        JsonNode productJson = objectMapper.readTree(kafkaPublishQueryInput.getJsonData());
        String authSecurityProtocol = ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.auth.security.protocol"));
        String saslMechanism = ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.sasl.mechanism"));
        String userName = ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.sasl.username"));
        String password = ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.sasl.password"));
        String endpoint = ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.endpoint"));

        Thread.currentThread().setContextClassLoader(this.getClass().getClassLoader());

        // Build Kafka properties
        Map<String, Object> kafkaProperties = new HashMap<>();
        kafkaProperties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, endpoint);
        kafkaProperties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");
        kafkaProperties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");

        // Add authentication if required
        if (!Objects.equals(authSecurityProtocol, "NONE")) {
            if ("certs".equals(ConfigEncryptionUtils.fromEnv().decryptProperty(
                    action.getContext().get("kafka.authentication.sasl.ssl.include")))) {
                setAuthenticationPropertiesWithCerts(authSecurityProtocol, kafkaProperties, saslMechanism, userName, password);
            } else {
                setAuthenticationProperties(authSecurityProtocol, kafkaProperties, saslMechanism, userName, password);
            }
        }

        String messageNode = objectMapper.writeValueAsString(productJson);
        KafkaProducer<String, String> producer = null;

        try {
            producer = new KafkaProducer<>(kafkaProperties);
            log.info("Producer created successfully on attempt {}", retryCount);

            // Send message
            ProducerRecord<String, String> producerRecord = new ProducerRecord<>(topicName, messageNode);
            AtomicReference<Exception> sendException = new AtomicReference<>();

            producer.send(producerRecord, (metadata, exception) -> {
                if (exception == null) {
                    log.info("Message sent to partition {} on attempt {}", metadata.partition(), retryCount);
                    audit.setPartition(metadata.partition());
                } else {
                    log.error(aMarker, "Send callback error: {}", exception.getMessage());
                    sendException.set(exception);
                }
            });

            producer.flush(); // Ensure message is sent

            if (sendException.get() != null) {
                throw sendException.get();
            }

            // Update audit with success after flush
            updateKafkaPublishAudit(-1, "SUCCESS", audit,
                    String.format("Success on attempt %d", retryCount));

        } finally {
            if (producer != null) {
                try {
                    producer.close();
                } catch (Exception closeEx) {
                    log.error(aMarker, "Error closing producer: {}", closeEx.getMessage());
                    HandymanException handymanException = new HandymanException(closeEx);
                    HandymanException.insertException("Error in closing producer", handymanException, action);
                }
            }
        }
    }

    private void setAuthenticationPropertiesWithCerts(String authSecurityProtocol, Map<String, Object> properties,
                                                      String saslMechanism, String userName, String password) {
        if (authSecurityProtocol.equalsIgnoreCase(SASL_SSL)) {
            properties.put("security.protocol", SASL_SSL);
            if (saslMechanism.equalsIgnoreCase(PLAIN_SASL)) {
                properties.put(SASL_MECHANISM, PLAIN_SASL);

                String jaasConfig = String.format("org.apache.kafka.common.security.plain.PlainLoginModule required username=\"%s\" password=\"%s\";", userName, password);
                properties.put("sasl.jaas.config", jaasConfig);
                properties.put("ssl.truststore.type", ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.truststore.type")));
                properties.put("ssl.keystore.type", ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.keystore.type")));
                properties.put("ssl.truststore.location", ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.truststore.location")));
                properties.put("ssl.truststore.password", ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.truststore.password")));
                properties.put("ssl.keystore.location", ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.keystore.location")));
                properties.put("ssl.keystore.password", ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.keystore.password")));
                properties.put("ssl.key.password", ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.key.password")));
                properties.put("ssl.endpoint.identification.algorithm", ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.endpoint.identification.algorithm")));
                properties.put(ProducerConfig.ACKS_CONFIG, ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.acks")));
                properties.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.request.timeout.ms")));
                properties.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.delivery.timeout.ms")));
                properties.put(ProducerConfig.LINGER_MS_CONFIG, ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.linger.ms")));
                boolean inBuildEnableRetry = Boolean.parseBoolean(action.getContext().getOrDefault(KAFKA_ENABLE_INBUILT_RETRY_KEY, "true"));
                if (inBuildEnableRetry) {
                    properties.put(ProducerConfig.RETRIES_CONFIG, ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.api.retries")));
                    properties.put(ProducerConfig.RETRY_BACKOFF_MS_CONFIG, ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.retry.backoff.multiplier")));
                    properties.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.enable.idempotence")));
                    properties.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.max.in.flight.requests")));
                }else{
                    properties.put(ProducerConfig.RETRIES_CONFIG, 0);}

            } else {
                if (saslMechanism.equalsIgnoreCase(SCRAM_SHA_256)) {
                    properties.put(SASL_MECHANISM, SCRAM_SHA_256);
                    String jaasConfig = String.format("org.apache.kafka.common.security.scram.ScramLoginModule required username=\"%s\" password=\"%s\";", userName, password);
                    properties.put("sasl.jaas.config", jaasConfig);
                } else {
                    if (saslMechanism.equalsIgnoreCase(OAUTH_BEARER)) {
                        properties.put(SASL_MECHANISM, OAUTH_BEARER);
                        properties.put("sasl.login.callback.handler.class", "your.oauth.LoginCallbackHandler");
                    }
                }
            }
        } else if (authSecurityProtocol.equalsIgnoreCase("SASL_PLAINTEXT")) {
            properties.put("security.protocol", "SASL_PLAINTEXT");
            properties.put(SASL_MECHANISM, PLAIN_SASL);
            String jaasConfig = String.format("org.apache.kafka.common.security.plain.PlainLoginModule required username=\"%s\" password=\"%s\";", userName, password);
            properties.put("sasl.jaas.config", jaasConfig);
            boolean enableRetry = Boolean.parseBoolean(action.getContext().getOrDefault(KAFKA_ENABLE_INBUILT_RETRY_KEY, "true"));
            if (enableRetry) {
                properties.put(ProducerConfig.ACKS_CONFIG, ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.acks")));
                properties.put(ProducerConfig.RETRIES_CONFIG, ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.api.retries")));
                properties.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.request.timeout.ms")));
                properties.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.delivery.timeout.ms")));
                properties.put(ProducerConfig.LINGER_MS_CONFIG, ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.linger.ms")));
                properties.put(ProducerConfig.RETRY_BACKOFF_MS_CONFIG, ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.retry.backoff.multiplier")));
                properties.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.enable.idempotence")));
                properties.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.max.in.flight.requests")));
            }else{
            properties.put(ProducerConfig.RETRIES_CONFIG, 0);}

        }
    }

    private static void setAuthenticationProperties(String authSecurityProtocol, Map<String, Object> properties,
                                                   String saslMechanism, String userName, String password) {
        if (authSecurityProtocol.equalsIgnoreCase(SASL_SSL)) {
            properties.put("security.protocol", SASL_SSL);
            if (saslMechanism.equalsIgnoreCase(PLAIN_SASL)) {
                properties.put(SASL_MECHANISM, PLAIN_SASL);
                String jaasConfig = String.format("org.apache.kafka.common.security.plain.PlainLoginModule required username=\"%s\" password=\"%s\";", userName, password);
                properties.put("sasl.jaas.config", jaasConfig);
            } else {
                if (saslMechanism.equalsIgnoreCase(SCRAM_SHA_256)) {
                    properties.put(SASL_MECHANISM, SCRAM_SHA_256);
                    String jaasConfig = String.format("org.apache.kafka.common.security.scram.ScramLoginModule required username=\"%s\" password=\"%s\";", userName, password);
                    properties.put("sasl.jaas.config", jaasConfig);
                } else {
                    if (saslMechanism.equalsIgnoreCase(OAUTH_BEARER)) {
                        properties.put(SASL_MECHANISM, OAUTH_BEARER);
                        properties.put("sasl.login.callback.handler.class", "your.oauth.LoginCallbackHandler");
                    }
                }
            }
        } else if (authSecurityProtocol.equalsIgnoreCase("SASL_PLAINTEXT")) {
            properties.put("security.protocol", "SASL_PLAINTEXT");
            properties.put(SASL_MECHANISM, PLAIN_SASL);
            String jaasConfig = String.format("org.apache.kafka.common.security.plain.PlainLoginModule required username=\"%s\" password=\"%s\";", userName, password);
            properties.put("sasl.jaas.config", jaasConfig);
        }
    }

    private KafkaProductionResponseAudit getKafkaPublishOutputAudit(KafkaPublishQueryInput kafkaPublishQueryInput,
                                                                    String topicName, String endpoint,
                                                                    String authSecurityProtocol, String saslMechanism) {
        KafkaProductionResponseAudit audit = new KafkaProductionResponseAudit();
        audit.setDocumentId(kafkaPublishQueryInput.getDocumentId());
        audit.setChecksum(kafkaPublishQueryInput.getFileChecksum());
        audit.setTenantId(kafkaPublishQueryInput.getTenantId());
        audit.setOriginId(kafkaPublishQueryInput.getOriginId());
        audit.setTopicName(topicName);
        audit.setEndpoint(endpoint);
        audit.setAuthSecurityProtocol(authSecurityProtocol);
        audit.setSaslMechanism(saslMechanism);
        audit.setBatchId(kafkaPublishQueryInput.getBatchId());
        audit.setTransactionId(kafkaPublishQueryInput.getTransactionId());
        audit.setCreatedUserId(kafkaPublishQueryInput.getTenantId());
        audit.setLastUpdatedUserId(kafkaPublishQueryInput.getTenantId());
        audit.setCreatedOn(LocalDateTime.now());
        audit.setVersion(1);
        return audit;
    }

    private void updateKafkaPublishAudit(int partition, String executionStatus,
                                         KafkaProductionResponseAudit kafkaPublishOutput,
                                         String response) {
        kafkaPublishOutput.setPartition(partition);
        kafkaPublishOutput.setExecStatus(executionStatus);
        kafkaPublishOutput.setLastUpdatedOn(LocalDateTime.now());
        kafkaPublishOutput.setResponse(response);
        kafkaPublishOutput.setStatus("ACTIVE");
    }

    private void insertExecutionInfoBatch(Jdbi jdbi, String outputTable,
                                          List<KafkaProductionResponseAudit> kafkaProductionResponseAudits) {
        if (kafkaProductionResponseAudits == null || kafkaProductionResponseAudits.isEmpty()) {
            log.warn("No KafkaProductionResponseAudit records to insert into table");
            return;
        }

        String sql = "INSERT INTO " + outputTable + " (" +
                "document_id, checksum, tenant_id, origin_id, batch_id, topic_name, endpoint, " +
                "auth_security_protocol, sasl_mechanism, response, partition, exec_status, " +
                "transaction_id, root_pipeline_id, created_user_id, last_updated_user_id, " +
                "created_on, last_updated_on, status, version, retry_count" +
                ") VALUES (" +
                ":documentId, :checksum, :tenantId, :originId, :batchId, :topicName, :endpoint, " +
                ":authSecurityProtocol, :saslMechanism, :response, :partition, :execStatus, " +
                ":transactionId, :rootPipelineId, :createdUserId, :lastUpdatedUserId, " +
                ":createdOn, :lastUpdatedOn, :status, :version, :retryCount" +
                ")";

        jdbi.useHandle(handle -> {
            PreparedBatch batch = handle.prepareBatch(sql);

            for (KafkaProductionResponseAudit record : kafkaProductionResponseAudits) {
                batch.bind("documentId", record.getDocumentId())
                        .bind("checksum", record.getChecksum())
                        .bind("tenantId", record.getTenantId())
                        .bind("originId", record.getOriginId())
                        .bind("batchId", record.getBatchId())
                        .bind("topicName", record.getTopicName())
                        .bind("endpoint", record.getEndpoint())
                        .bind("authSecurityProtocol", record.getAuthSecurityProtocol())
                        .bind("saslMechanism", record.getSaslMechanism())
                        .bind("response", record.getResponse())
                        .bind("partition", record.getPartition())
                        .bind("execStatus", record.getExecStatus())
                        .bind("transactionId", record.getTransactionId())
                        .bind("rootPipelineId", record.getRootPipelineId())
                        .bind("createdUserId", record.getCreatedUserId())
                        .bind("lastUpdatedUserId", record.getLastUpdatedUserId())
                        .bind("createdOn", record.getCreatedOn())
                        .bind("lastUpdatedOn", record.getLastUpdatedOn())
                        .bind("status", record.getStatus())
                        .bind("version", record.getVersion())
                        .bind("retryCount", record.getRetryCount())
                        .add();
            }

            int[] results = batch.execute();
            log.info("Batch insert completed into [{}]. Total Records: {}, Successful Inserts: {}",
                    outputTable, kafkaProductionResponseAudits.size(), results.length);
        });
    }

    @Data
    @AllArgsConstructor
    @NoArgsConstructor
    @Builder
    public static class KafkaProductionResponseAudit {
        private String documentId;
        private String checksum;
        private Long tenantId;
        private String originId;
        private String batchId;
        private String topicName;
        private String endpoint;
        private String authSecurityProtocol;
        private String saslMechanism;
        private String response;
        private Integer partition;
        private String execStatus;
        private String transactionId;
        private Long rootPipelineId;
        private Long createdUserId;
        private Long lastUpdatedUserId;
        private LocalDateTime createdOn;
        private LocalDateTime lastUpdatedOn;
        private String status;
        private Integer version;
        private Integer retryCount;
    }

    @Data
    @AllArgsConstructor
    @NoArgsConstructor
    @Builder
    public static class KafkaPublishQueryInput {
        private String endpoint;
        private String topicName;
        private String authSecurityProtocol;
        private String saslMechanism;
        private String userName;
        private String password;
        private String encryptionType;
        private String encryptionKey;
        private String jsonData;
        private String documentId;
        private String fileChecksum;
        private String originId;
        private String batchId;
        private Long tenantId;
        private String transactionId;
    }

    @Override
    public boolean executeIf() throws Exception {
        return kafkaPublish.getCondition();
    }
}