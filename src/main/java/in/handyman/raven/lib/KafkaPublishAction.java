package in.handyman.raven.lib;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import in.handyman.raven.core.utils.ConfigEncryptionUtils;
import in.handyman.raven.exception.HandymanException;
import in.handyman.raven.lambda.access.ResourceAccess;
import in.handyman.raven.lambda.action.ActionExecution;
import in.handyman.raven.lambda.action.IActionExecution;
import in.handyman.raven.lambda.doa.audit.ActionExecutionAudit;
import in.handyman.raven.lib.model.KafkaPublish;
import in.handyman.raven.util.CommonQueryUtil;
import in.handyman.raven.util.EncryptDecrypt;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.Metric;
import org.apache.kafka.common.MetricName;
import org.jdbi.v3.core.Jdbi;
import org.jdbi.v3.core.result.ResultIterable;
import org.jdbi.v3.core.statement.PreparedBatch;
import org.jdbi.v3.core.statement.Query;
import org.slf4j.Logger;
import org.slf4j.Marker;
import org.slf4j.MarkerFactory;

import javax.crypto.BadPaddingException;
import javax.crypto.IllegalBlockSizeException;
import javax.crypto.NoSuchPaddingException;
import java.security.InvalidKeyException;
import java.security.NoSuchAlgorithmException;
import java.time.LocalDateTime;
import java.util.*;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.stream.Collectors;

/**
 * Auto Generated By Raven
 */
@ActionExecution(
        actionName = "KafkaPublish"
)
public class KafkaPublishAction implements IActionExecution {
    private final ActionExecutionAudit action;

    private final Logger log;

    private final KafkaPublish kafkaPublish;

    private final Marker aMarker;
    private ObjectMapper objectMapper;
    private static final String SASL_MECHANISM = "sasl.mechanism";
    private static final String SASL_SSL = "SASL_SSL";
    private static final String SCRAM_SHA_256 = "SCRAM-SHA-256";
    private static final String OAUTH_BEARER = "OAUTHBEARER";
    private static final String PLAIN_SASL = "PLAIN";
    private static final String AES_ENCRYPTION = "AES";
    private static final String FAILED_STATUS = "FAILED";
    private static final String ENABLE_RETRY_KEY = "kafka.enable.retry";

    public KafkaPublishAction(final ActionExecutionAudit action, final Logger log,
                              final Object kafkaPublish) {
        this.kafkaPublish = (KafkaPublish) kafkaPublish;
        this.action = action;
        this.log = log;
        this.aMarker = MarkerFactory.getMarker(" KafkaPublish:" + this.kafkaPublish.getName());
    }


    @Override
    public void execute() throws Exception {

        try {
            objectMapper = new ObjectMapper();
            final Jdbi jdbi = ResourceAccess.rdbmsJDBIConn(kafkaPublish.getResourceConn());
            final List<KafkaPublishQueryInput> kafkaPublishQueryInputs = new ArrayList<>();

            final List<KafkaProductionResponseAudit> auditRecords = new ArrayList<>();

            String outputTable = kafkaPublish.getOutputTable();

            jdbi.useTransaction(handle -> handle.execute("create table if not exists " + outputTable +
                    " (id bigserial not null, document_id varchar, checksum varchar, tenant_id int8, " +
                    "origin_id varchar, batch_id varchar, topic_name varchar, endpoint varchar, " +
                    "auth_security_protocol varchar, sasl_mechanism varchar, response varchar, " +
                    "partition  int8, exec_status varchar, transaction_id varchar, root_pipeline_id  int8, " +
                    "created_user_id  int8, last_updated_user_id  int8, created_on timestamp default now(), " +
                    "last_updated_on timestamp, status varchar, version int, retry_count int default 0);"));

            jdbi.useTransaction(handle -> {
                final List<String> formattedQuery = CommonQueryUtil.getFormattedQuery(kafkaPublish.getQuerySet());
                AtomicInteger i = new AtomicInteger(0);
                formattedQuery.forEach(sqlToExecute -> {
                    log.info(aMarker, "executing  query {} from index {}", sqlToExecute, i.getAndIncrement());
                    Query query = handle.createQuery(sqlToExecute);
                    ResultIterable<KafkaPublishQueryInput> resultIterable = query.mapToBean(KafkaPublishQueryInput.class);
                    List<KafkaPublishQueryInput> publishQueryInputs = resultIterable.stream().collect(Collectors.toList());
                    kafkaPublishQueryInputs.addAll(publishQueryInputs);
                    log.info(aMarker, "executed query from index {}", i.get());
                });
            });
            int inputQueryObjectSize = kafkaPublishQueryInputs.size();
            log.info(aMarker, "Input Query output has {} objects present", inputQueryObjectSize);
            kafkaPublishQueryInputs.forEach(kafkaPublishQueryInput -> {
                try {
                    KafkaProductionResponseAudit audit = doKafkaPublish(kafkaPublishQueryInput, jdbi, outputTable);
                    if (audit != null) {
                        auditRecords.add(audit);
                    }
                } catch (JsonProcessingException e) {
                    log.error(aMarker, "Error processing kafka publish query input", e);
                    HandymanException handymanException = new HandymanException(e);
                    HandymanException.insertException("Error processing kafka publish query input", handymanException, action);
                    throw new RuntimeException(e);
                }
            });

            if (!auditRecords.isEmpty()) {
                insertExecutionInfoBatch(jdbi, outputTable, auditRecords);
                log.info(aMarker, "Batch inserted {} audit records", auditRecords.size());
            }

        } catch (Exception ex) {
            log.error(aMarker, "Error in execute method for kafka publish Action", ex);
            HandymanException handymanException = new HandymanException(ex);
            HandymanException.insertException("Error in execute method for kafka publish Action", handymanException, action);
            throw new HandymanException("Error in execute method for kafka publish Action", ex, action);
        }
    }

    private KafkaProductionResponseAudit doKafkaPublish(KafkaPublishQueryInput kafkaPublishQueryInput,
                                                        Jdbi jdbi, String outputTable) throws JsonProcessingException {
        log.info("processing the kafka input data ");

        String topicName = ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.topic.name"));
        String responseNode = kafkaPublishQueryInput.getJsonData();
        JsonNode productJson = objectMapper.readTree(responseNode);
        String authSecurityProtocol = ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.auth.security.protocol"));
        String saslMechanism = ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.sasl.mechanism"));

        String userName = ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.sasl.username"));
        String password = ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.sasl.password"));
        String endpoint = ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.endpoint"));

        KafkaProductionResponseAudit audit = getKafkaPublishOutputAudit(kafkaPublishQueryInput);

        Thread.currentThread().setContextClassLoader(this.getClass().getClassLoader());

        Map<String, Object> kafkaProperties = new HashMap<>();
        kafkaProperties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, endpoint);
        kafkaProperties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");
        kafkaProperties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");

        boolean retryEnabled = Boolean.parseBoolean(
                action.getContext().getOrDefault(ENABLE_RETRY_KEY, "true"));

        if (retryEnabled) {
            int maxRetries = Integer.parseInt(action.getContext().getOrDefault("kafka.max.retry.attempts", "3"));
            long retryBackoffMs = Long.parseLong(action.getContext().getOrDefault("kafka.retry.backoff.ms", "100"));

            kafkaProperties.put(ProducerConfig.RETRIES_CONFIG, maxRetries);
            kafkaProperties.put(ProducerConfig.RETRY_BACKOFF_MS_CONFIG, retryBackoffMs);

            log.info("Kafka retry enabled: retries={}, backoffMs={}", maxRetries, retryBackoffMs);
        } else {
            kafkaProperties.put(ProducerConfig.RETRIES_CONFIG, 0);
            log.info("Kafka retry disabled");
        }
        if (Objects.equals(authSecurityProtocol, "NONE")) {
            try {


                String messageNode = "";
                try {
                    messageNode = objectMapper.writeValueAsString(productJson);
                } catch (Exception e) {
                    log.error(aMarker, "Error in converting json data to kafka topic message {}", authSecurityProtocol, e);
                    HandymanException handymanException = new HandymanException(e);
                    HandymanException.insertException("Error in converting json data to kafka topic message", handymanException, action);
                }
                KafkaProducer<String, String> producer = new KafkaProducer<>(kafkaProperties);
                log.info("Creating the kafka producer instance");

                ProducerRecord<String, String> producerRecord = new ProducerRecord<>(topicName, messageNode);

                // ✅ CAPTURE METRICS BEFORE AND AFTER
                int retriesBeforeSend = getKafkaRetryCount(producer);

                try {
                    producer.send(producerRecord, (metadata, exception) -> {
                        // ✅ CAPTURE RETRY COUNT IN CALLBACK
                        int retriesAfterSend = getKafkaRetryCount(producer);
                        int actualRetries = retriesAfterSend - retriesBeforeSend;

                        if (exception == null) {
                            log.info("✅ Message sent successfully after {} retries", actualRetries);
                            int partition = metadata.partition();
                            audit.setRetryCount(actualRetries);
                            updateKafkaPublishAudit(partition, "SUCCESS", audit,
                                    String.format("Success after %d retries", actualRetries));
                        } else {
                            log.error(aMarker, "❌ Failed after {} retries: {}", actualRetries, exception.getMessage());
                            audit.setRetryCount(actualRetries);
                            updateKafkaPublishAudit(-1, FAILED_STATUS, audit,
                                    String.format("Failed after %d retries: %s", actualRetries, exception.getMessage()));
                            HandymanException handymanException = new HandymanException(exception);
                            HandymanException.insertException("Error in publishing message to kafka topic", handymanException, action);
                        }
                    });

                } catch (Exception e) {
                    int retriesAfterSend = getKafkaRetryCount(producer);
                    int actualRetries = retriesAfterSend - retriesBeforeSend;
                    log.error(aMarker, "Exception after {} retries", actualRetries, e);
                    audit.setRetryCount(actualRetries);
                    updateKafkaPublishAudit(-1, FAILED_STATUS, audit,
                            String.format("Exception after %d retries: %s", actualRetries, e.getMessage()));
                } finally {
                    log.info("Closing Producer");
                    producer.close();
                }
            } catch (Exception e) {
                log.error(aMarker, "Unexpected error in kafka publish", e);
                updateKafkaPublishAudit(-1, FAILED_STATUS, audit, "Unexpected error: " + e.getMessage());
                HandymanException handymanException = new HandymanException(e);
                HandymanException.insertException("Unexpected error in kafka publish", handymanException, action);
            }
        } else {

            try {
                log.info("Processing kafka with authentication");

                if (ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.authentication.sasl.ssl.include")).equals("certs")) {
                    setAuthenticationPropertiesWithCerts(authSecurityProtocol, kafkaProperties, saslMechanism, userName, password);

                } else {
                    setAuthenticationProperties(authSecurityProtocol, kafkaProperties, saslMechanism, userName, password);

                }
                KafkaProducer<String, String> producer = new KafkaProducer<>(kafkaProperties);
                log.info("Creating the kafka instance with authentication");

                String messageNode = "";
                try {
                    messageNode = objectMapper.writeValueAsString(productJson);
                } catch (Exception e) {
                    log.error(aMarker, "Error in converting json data to kafka topic message", e);
                    updateKafkaPublishAudit(-1, FAILED_STATUS, audit, "Error converting JSON: " + e.getMessage());
                    HandymanException handymanException = new HandymanException(e);
                    HandymanException.insertException("Error in converting json data to kafka topic message", handymanException, action);
                    return audit;
                }


                ProducerRecord<String, String> producerRecord = new ProducerRecord<>(topicName, messageNode);
                log.info("Sending the topic and output json");

                // ✅ CAPTURE METRICS BEFORE AND AFTER
                int retriesBeforeSend = getKafkaRetryCount(producer);

                try {
                    producer.send(producerRecord, (metadata, exception) -> {
                        int retriesAfterSend = getKafkaRetryCount(producer);
                        int actualRetries = retriesAfterSend - retriesBeforeSend;

                        if (exception == null) {
                            log.info("✅ Message sent successfully after {} retries", actualRetries);
                            int partition = metadata.partition();
                            audit.setRetryCount(actualRetries);
                            updateKafkaPublishAudit(partition, "SUCCESS", audit,
                                    String.format("Success after %d retries", actualRetries));
                        } else {
                            log.error(aMarker, "❌ Failed after {} retries: {}", actualRetries, exception.getMessage());
                            audit.setRetryCount(actualRetries);
                            updateKafkaPublishAudit(-1, FAILED_STATUS, audit,
                                    String.format("Failed after %d retries: %s", actualRetries, exception.getMessage()));
                            HandymanException handymanException = new HandymanException(exception);
                            HandymanException.insertException("Error in publishing message to kafka topic", handymanException, action);
                        }
                    }); // Wait for completion

                } catch (Exception e) {
                    int retriesAfterSend = getKafkaRetryCount(producer);
                    int actualRetries = retriesAfterSend - retriesBeforeSend;
                    log.error(aMarker, "Exception after {} retries", actualRetries, e);
                    audit.setRetryCount(actualRetries);
                    updateKafkaPublishAudit(-1, FAILED_STATUS, audit,
                            String.format("Exception after %d retries: %s", actualRetries, e.getMessage()));
                } finally {
                    producer.close();
                }
            } catch (Exception e) {
                log.error(aMarker, "Unexpected error in kafka publish", e);
                updateKafkaPublishAudit(-1, FAILED_STATUS, audit, "Unexpected error: " + e.getMessage());
                HandymanException handymanException = new HandymanException(e);
                HandymanException.insertException("Unexpected error in kafka publish", handymanException, action);
            }
        }

        return audit;
    }

    private String doOptionalMessageEncryption(String messageNode, String encryptionType, String encryptionKey) throws NoSuchPaddingException, IllegalBlockSizeException, NoSuchAlgorithmException, BadPaddingException, InvalidKeyException {
        String message = EncryptDecrypt.encrypt(messageNode, encryptionKey, encryptionType);
        log.info("Encrypted message using algorithm {}", encryptionType);
        return message;

    }

    private void setAuthenticationPropertiesWithCerts(String authSecurityProtocol, Map<String, Object> properties, String saslMechanism, String userName, String password) {
        if (authSecurityProtocol.equalsIgnoreCase(SASL_SSL)) {
            properties.put("security.protocol", SASL_SSL);
            if (saslMechanism.equalsIgnoreCase(PLAIN_SASL)) {
                properties.put(SASL_MECHANISM, PLAIN_SASL);

                String jaasConfig = String.format("org.apache.kafka.common.security.plain.PlainLoginModule required username=\"%s\" password=\"%s\";", userName, password);
                properties.put("sasl.jaas.config", jaasConfig);
                properties.put("ssl.truststore.type", ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.truststore.type")));
                properties.put("ssl.keystore.type", ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.keystore.type")));
                properties.put("ssl.truststore.location", ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.truststore.location")));
                properties.put("ssl.truststore.password", ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.truststore.password")));
                properties.put("ssl.keystore.location", ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.keystore.location")));
                properties.put("ssl.keystore.password", ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.keystore.password")));
                properties.put("ssl.key.password", ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.key.password")));
                properties.put("ssl.endpoint.identification.algorithm", ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.endpoint.identification.algorithm")));
                properties.put(ProducerConfig.ACKS_CONFIG, ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.acks")));
                properties.put(ProducerConfig.RETRIES_CONFIG, ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.api.retries")));
                properties.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.request.timeout.ms")));
                properties.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.delivery.timeout.ms")));  // 5 minutes
                properties.put(ProducerConfig.LINGER_MS_CONFIG, ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.linger.ms")));
            } else {
                if (saslMechanism.equalsIgnoreCase(SCRAM_SHA_256)) {
                    properties.put(SASL_MECHANISM, SCRAM_SHA_256);
                    String jaasConfig = String.format("org.apache.kafka.common.security.scram.ScramLoginModule required username=\"%s\" password=\"%s\";", userName, password);
                    properties.put("sasl.jaas.config", jaasConfig);
                } else {
                    if (saslMechanism.equalsIgnoreCase(OAUTH_BEARER)) {
                        properties.put(SASL_MECHANISM, OAUTH_BEARER);
                        properties.put("sasl.login.callback.handler.class", "your.oauth.LoginCallbackHandler");
                    }
                }
            }
        } else if (authSecurityProtocol.equalsIgnoreCase("SASL_PLAINTEXT")) {
            properties.put("security.protocol", "SASL_PLAINTEXT");
            properties.put(SASL_MECHANISM, PLAIN_SASL);
            String jaasConfig = String.format("org.apache.kafka.common.security.plain.PlainLoginModule required username=\"%s\" password=\"%s\";", userName, password);
            properties.put("sasl.jaas.config", jaasConfig);
//            properties.put("sasl.jaas.config",
//                    "org.apache.kafka.common.security.plain.PlainLoginModule required " +
//                            "username=\"" + userName + "\" " +
//                            "password=\"" + password + "\";");

            properties.put(ProducerConfig.ACKS_CONFIG, ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.acks")));
            properties.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.request.timeout.ms")));
            properties.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.delivery.timeout.ms")));
            properties.put(ProducerConfig.LINGER_MS_CONFIG, ConfigEncryptionUtils.fromEnv().decryptProperty(action.getContext().get("kafka.ssl.linger.ms")));

            log.info(aMarker, "SASL_PLAINTEXT authentication configured successfully");
        }
    }


    private static void setAuthenticationProperties(String authSecurityProtocol, Map<String, Object> properties, String saslMechanism, String userName, String password) {
        if (authSecurityProtocol.equalsIgnoreCase(SASL_SSL)) {
            properties.put("security.protocol", SASL_SSL);
            if (saslMechanism.equalsIgnoreCase(PLAIN_SASL)) {
                properties.put(SASL_MECHANISM, PLAIN_SASL);
                String jaasConfig = String.format("org.apache.kafka.common.security.plain.PlainLoginModule required username=\"%s\" password=\"%s\";", userName, password);
                properties.put("sasl.jaas.config", jaasConfig);
            } else {
                if (saslMechanism.equalsIgnoreCase(SCRAM_SHA_256)) {
                    properties.put(SASL_MECHANISM, SCRAM_SHA_256);
                    String jaasConfig = String.format("org.apache.kafka.common.security.scram.ScramLoginModule required username=\"%s\" password=\"%s\";", userName, password);
                    properties.put("sasl.jaas.config", jaasConfig);
                } else {
                    if (saslMechanism.equalsIgnoreCase(OAUTH_BEARER)) {
                        properties.put(SASL_MECHANISM, OAUTH_BEARER);
                        properties.put("sasl.login.callback.handler.class", "your.oauth.LoginCallbackHandler");
                    }
                }
            }
        } else if (authSecurityProtocol.equalsIgnoreCase("SASL_PLAINTEXT")) {
            properties.put("security.protocol", "SASL_PLAINTEXT");
            properties.put(SASL_MECHANISM, PLAIN_SASL);
            String jaasConfig = String.format("org.apache.kafka.common.security.plain.PlainLoginModule required username=\"%s\" password=\"%s\";", userName, password);
            properties.put("sasl.jaas.config", jaasConfig);
//            properties.put("sasl.jaas.config",
//                    "org.apache.kafka.common.security.plain.PlainLoginModule required " +
//                            "username=\"" + userName + "\" " +
//                            "password=\"" + password + "\";");

        }
    }


    private KafkaProductionResponseAudit getKafkaPublishOutputAudit(KafkaPublishQueryInput kafkaPublishQueryInput) {
        KafkaProductionResponseAudit kafkaProductionResponseAudit = new KafkaProductionResponseAudit();

        kafkaProductionResponseAudit.setDocumentId(kafkaPublishQueryInput.getDocumentId());
        kafkaProductionResponseAudit.setChecksum(kafkaPublishQueryInput.getFileChecksum());
        kafkaProductionResponseAudit.setTenantId(kafkaPublishQueryInput.getTenantId());
        kafkaProductionResponseAudit.setOriginId(kafkaPublishQueryInput.getOriginId());
        kafkaProductionResponseAudit.setTopicName(kafkaPublishQueryInput.getTopicName());
        kafkaProductionResponseAudit.setEndpoint(kafkaPublishQueryInput.getEndpoint());
        kafkaProductionResponseAudit.setAuthSecurityProtocol(kafkaPublishQueryInput.getAuthSecurityProtocol());
        kafkaProductionResponseAudit.setSaslMechanism(kafkaPublishQueryInput.getSaslMechanism());
        kafkaProductionResponseAudit.setBatchId(kafkaPublishQueryInput.getBatchId());
        kafkaProductionResponseAudit.setTransactionId(kafkaPublishQueryInput.getTransactionId());
        kafkaProductionResponseAudit.setCreatedUserId(kafkaPublishQueryInput.getTenantId());
        kafkaProductionResponseAudit.setLastUpdatedUserId(kafkaPublishQueryInput.getTenantId());
        kafkaProductionResponseAudit.setCreatedOn(LocalDateTime.now());
        kafkaProductionResponseAudit.setVersion(1);
        kafkaProductionResponseAudit.setRetryCount(0);

        return kafkaProductionResponseAudit;
    }

    private void updateKafkaPublishAudit(int partition, String executionStatus,
                                         KafkaProductionResponseAudit kafkaPublishOutput,
                                         String response) {
        kafkaPublishOutput.setPartition(partition);
        kafkaPublishOutput.setExecStatus(executionStatus);
        kafkaPublishOutput.setLastUpdatedOn(LocalDateTime.now());
        kafkaPublishOutput.setResponse(response);
        kafkaPublishOutput.setStatus("ACTIVE");
    }

    private void insertExecutionInfoBatch(Jdbi jdbi, String outputTable,
                                          List<KafkaProductionResponseAudit> kafkaProductionResponseAudits) {
        if (kafkaProductionResponseAudits == null || kafkaProductionResponseAudits.isEmpty()) {
            log.warn("No KafkaProductionResponseAudit records to insert into table");
            return;
        }

        String sql = "INSERT INTO " + outputTable + " (" +
                "document_id, checksum, tenant_id, origin_id, batch_id, topic_name, endpoint, " +
                "auth_security_protocol, sasl_mechanism, response, partition, exec_status, " +
                "transaction_id, root_pipeline_id, created_user_id, last_updated_user_id, " +
                "created_on, last_updated_on, status, version, retry_count" +
                ") VALUES (" +
                ":documentId, :checksum, :tenantId, :originId, :batchId, :topicName, :endpoint, " +
                ":authSecurityProtocol, :saslMechanism, :response, :partition, :execStatus, " +
                ":transactionId, :rootPipelineId, :createdUserId, :lastUpdatedUserId, " +
                ":createdOn, :lastUpdatedOn, :status, :version, :retryCount" +
                ")";

        jdbi.useHandle(handle -> {
            PreparedBatch batch = handle.prepareBatch(sql);

            for (KafkaProductionResponseAudit record : kafkaProductionResponseAudits) {
                batch.bind("documentId", record.getDocumentId())
                        .bind("checksum", record.getChecksum())
                        .bind("tenantId", record.getTenantId())
                        .bind("originId", record.getOriginId())
                        .bind("batchId", record.getBatchId())
                        .bind("topicName", record.getTopicName())
                        .bind("endpoint", record.getEndpoint())
                        .bind("authSecurityProtocol", record.getAuthSecurityProtocol())
                        .bind("saslMechanism", record.getSaslMechanism())
                        .bind("response", record.getResponse())
                        .bind("partition", record.getPartition())
                        .bind("execStatus", record.getExecStatus())
                        .bind("transactionId", record.getTransactionId())
                        .bind("rootPipelineId", record.getRootPipelineId())
                        .bind("createdUserId", record.getCreatedUserId())
                        .bind("lastUpdatedUserId", record.getLastUpdatedUserId())
                        .bind("createdOn", record.getCreatedOn())
                        .bind("lastUpdatedOn", record.getLastUpdatedOn())
                        .bind("status", record.getStatus())
                        .bind("version", record.getVersion())
                        .bind("retryCount", record.getRetryCount())
                        .add();
            }

            int[] results = batch.execute();
            log.info("✅ Batch insert completed into [{}]. Total Records: {}, Successful Inserts: {}",
                    outputTable, kafkaProductionResponseAudits.size(), results.length);
        });
    }
    private int getKafkaRetryCount(KafkaProducer<String, String> producer) {
        try {
            Map<MetricName, ? extends Metric> metrics = producer.metrics();

            // Look for record-retry-total metric
            for (Map.Entry<MetricName, ? extends Metric> entry : metrics.entrySet()) {
                MetricName metricName = entry.getKey();
                if ("record-retry-total".equals(metricName.name()) ||
                        "record-retry-rate".equals(metricName.name())) {
                    double value = (double) entry.getValue().metricValue();
                    log.debug("Found retry metric: {} = {}", metricName.name(), value);
                    return (int) value;
                }
            }
        } catch (Exception e) {
            log.warn("Could not retrieve retry metrics: {}", e.getMessage());
            HandymanException handymanException = new HandymanException(e);
            HandymanException.insertException("Could not retrieve retry metrics", handymanException, action);
        }
        return 0;
    }


    @Data
    @AllArgsConstructor
    @NoArgsConstructor
    @Builder
    public static class KafkaProductionResponseAudit {
        private String documentId;
        private String checksum;
        private Long tenantId;
        private String originId;
        private String batchId;
        private String topicName;
        private String endpoint;
        private String authSecurityProtocol;
        private String saslMechanism;
        private String response;
        private Integer partition;
        private String execStatus;
        private String transactionId;
        private Long rootPipelineId;
        private Long createdUserId;
        private Long lastUpdatedUserId;
        private LocalDateTime createdOn;
        private LocalDateTime lastUpdatedOn;
        private String status;
        private Integer version;
        private Integer retryCount;
    }
    @Data
    @AllArgsConstructor
    @NoArgsConstructor
    @Builder
    public static class KafkaPublishQueryInput {
        private String endpoint;
        private String topicName;
        private String authSecurityProtocol;
        private String saslMechanism;
        private String userName;
        private String password;
        private String encryptionType;
        private String encryptionKey;
        private String jsonData;
        private String documentId;
        private String fileChecksum;
        private String originId;
        private String batchId;
        private Long tenantId;
        private String transactionId;
    }

    @Override
    public boolean executeIf() throws Exception {
        return kafkaPublish.getCondition();
    }
}