package in.handyman.raven.lib;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.node.ObjectNode;
import in.handyman.raven.exception.HandymanException;
import in.handyman.raven.lambda.access.ResourceAccess;
import in.handyman.raven.lambda.action.ActionExecution;
import in.handyman.raven.lambda.action.IActionExecution;
import in.handyman.raven.lambda.doa.audit.ActionExecutionAudit;
import in.handyman.raven.lib.model.KafkaPublish;
import in.handyman.raven.util.CommonQueryUtil;
import in.handyman.raven.util.EncryptDecrypt;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.jdbi.v3.core.Jdbi;
import org.jdbi.v3.core.result.ResultIterable;
import org.jdbi.v3.core.statement.Query;
import org.slf4j.Logger;
import org.slf4j.Marker;
import org.slf4j.MarkerFactory;

import javax.crypto.BadPaddingException;
import javax.crypto.IllegalBlockSizeException;
import javax.crypto.NoSuchPaddingException;
import java.security.InvalidKeyException;
import java.security.NoSuchAlgorithmException;
import java.util.*;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.stream.Collectors;

/**
 * Auto Generated By Raven
 */
@ActionExecution(
        actionName = "KafkaPublish"
)
public class KafkaPublishAction implements IActionExecution {
    private final ActionExecutionAudit action;

    private final Logger log;

    private final KafkaPublish kafkaPublish;

    private final Marker aMarker;
    private ObjectMapper objectMapper;

    public KafkaPublishAction(final ActionExecutionAudit action, final Logger log,
                              final Object kafkaPublish) {
        this.kafkaPublish = (KafkaPublish) kafkaPublish;
        this.action = action;
        this.log = log;
        this.aMarker = MarkerFactory.getMarker(" KafkaPublish:" + this.kafkaPublish.getName());
    }

    private static final String SASL_MECHANISM = "sasl.mechanism";
    private static final String SASL_SSL = "SASL_SSL";
    private static final String SCRAM_SHA_256 = "SCRAM-SHA-256";
    private static final String OAUTH_BEARER = "OAUTHBEARER";
    private static final String PLAIN_SASL = "PLAIN";
    private static final String AES_ENCRYPTION = "AES";
    private static final String FAILED_STATUS = "FAILED";

    @Override
    public void execute() throws Exception {

        try {
            objectMapper=new ObjectMapper();
            final Jdbi jdbi = ResourceAccess.rdbmsJDBIConn(kafkaPublish.getResourceConn());
            final List<KafkaPublishQueryInput> kafkaPublishQueryInputs = new ArrayList<>();

            String outputTable = kafkaPublish.getOutputTable();
            jdbi.useTransaction(handle -> handle.execute("create table if not exists product_outbound." + outputTable + " (id bigserial not null, document_id varchar, checksum varchar, tenant_id int8, origin_id varchar, batch_id varchar, topic_name varchar, endpoint varchar, auth_security_protocol varchar, sasl_mechanism varchar, response varchar, partition varchar, exec_status varchar, created_on timestamp default now(), transaction_id varchar);"));

            jdbi.useTransaction(handle -> {
                final List<String> formattedQuery = CommonQueryUtil.getFormattedQuery(kafkaPublish.getQuerySet());
                AtomicInteger i = new AtomicInteger(0);
                formattedQuery.forEach(sqlToExecute -> {
                    log.info(aMarker, "executing  query {} from index {}", sqlToExecute, i.getAndIncrement());
                    Query query = handle.createQuery(sqlToExecute);
                    ResultIterable<KafkaPublishQueryInput> resultIterable = query.mapToBean(KafkaPublishQueryInput.class);
                    List<KafkaPublishQueryInput> publishQueryInputs = resultIterable.stream().collect(Collectors.toList());
                    kafkaPublishQueryInputs.addAll(publishQueryInputs);
                    log.info(aMarker, "executed query from index {}", i.get());
                });
            });

            kafkaPublishQueryInputs.forEach(kafkaPublishQueryInput -> {
                try {
                    doKafkaPublish(kafkaPublishQueryInput, jdbi, outputTable);
                } catch (JsonProcessingException e) {
                    throw new RuntimeException(e);
                }
            });
        } catch (Exception ex) {
            log.error(aMarker, "Error in execute method for kafka publish Action", ex);
            throw new HandymanException("Error in execute method for kafka publish Action", ex, action);
        }
    }

    private void doKafkaPublish(KafkaPublishQueryInput kafkaPublishQueryInput, Jdbi jdbi, String outputTable) throws JsonProcessingException {
      log.info("processing the kafka input data kafkaPublishQueryInput: {}", kafkaPublishQueryInput);
        String documentId = kafkaPublishQueryInput.getDocumentId();
        Long tenantId = kafkaPublishQueryInput.getTenantId();
        String transactionId = kafkaPublishQueryInput.getTransactionId();
        String originId = kafkaPublishQueryInput.getOriginId();
        String batchId = kafkaPublishQueryInput.getBatchId();

        String topicName = kafkaPublishQueryInput.getTopicName();
        String responseNode = kafkaPublishQueryInput.getJsonData();
        JsonNode productJson = objectMapper.readTree(responseNode);
        String fileChecksum = kafkaPublishQueryInput.getFileChecksum();
        String authSecurityProtocol = kafkaPublishQueryInput.getAuthSecurityProtocol();
        String saslMechanism = kafkaPublishQueryInput.getSaslMechanism();

        String userName = kafkaPublishQueryInput.getUserName();
        String password = kafkaPublishQueryInput.getPassword();
        String endpoint = kafkaPublishQueryInput.getEndpoint();

        Thread.currentThread().setContextClassLoader(this.getClass().getClassLoader());

        Map<String,Object> kafkaProperties=new HashMap<>();
        kafkaProperties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,endpoint);
        kafkaProperties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,"org.apache.kafka.common.serialization.StringSerializer");
        kafkaProperties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,"org.apache.kafka.common.serialization.StringSerializer");



        if(Objects.equals(authSecurityProtocol, "NONE")){
            try {


                ObjectNode objectNode = objectMapper.createObjectNode();
                objectNode.put("documentId", documentId);
                objectNode.put("checksum", fileChecksum);
                objectNode.put("transactionId", transactionId);
                objectNode.set("extractionResponse", productJson);

                String messageNode;
                try {
                    messageNode = objectMapper.writeValueAsString(objectNode);
                } catch (Exception e) {
                    log.error(aMarker, "Error in converting json data to kafka topic message", e);
                    insertExecutionInfo(jdbi, outputTable, documentId, fileChecksum, tenantId, originId, batchId, topicName, endpoint, authSecurityProtocol, saslMechanism, e.getMessage(), -1, FAILED_STATUS, transactionId);
                    throw new HandymanException("Error in converting json data to kafka topic message", e, action);
                }
                KafkaProducer<String, String> producer = new KafkaProducer<>(kafkaProperties);
                log.info( "creating the kafka instance");
                ProducerRecord<String, String> producerRecord = new ProducerRecord<>(topicName, messageNode);
                log.info("The sending outputJson to kafka: {}", producerRecord);
                try {
                    producer.send(producerRecord, (metadata, exception) -> {
                        if (exception == null) {
                            log.info("Successful in sending the message to kafka topic");
                            int partition = metadata.partition();
                            log.info("Topic: {}, Partition: {}", metadata.topic(), partition);
                            insertExecutionInfo(jdbi, outputTable, documentId, fileChecksum, tenantId, originId, batchId, topicName, endpoint, authSecurityProtocol, saslMechanism, "Successful in sending message to topic", partition, "SUCCESS", transactionId);
                        } else {
                            log.error(aMarker, "Error in publishing message to kafka topic", exception);
                            insertExecutionInfo(jdbi, outputTable, documentId, fileChecksum, tenantId, originId, batchId, topicName, endpoint, authSecurityProtocol, saslMechanism, exception.getMessage(), -1, FAILED_STATUS, transactionId);
                            throw new HandymanException("Error in publishing message to kafka topic", exception, action);
                        }
                    });
                } finally {
                    log.info("Closing Producer");
                    producer.close();
                }
            }
            catch (HandymanException e) {
                log.info("Error in posting kafka topic message");
                insertExecutionInfo(jdbi, outputTable, documentId, fileChecksum, tenantId, originId, batchId, topicName, endpoint, authSecurityProtocol, saslMechanism, e.getMessage(), -1, FAILED_STATUS, transactionId);
                throw new HandymanException("Error in posting kafka topic message", e, action);
            }
        }
        else{

            try {
                log.info("the kafka input data binding");
                String encryptionType = kafkaPublishQueryInput.getEncryptionType();
                String encryptionKey = kafkaPublishQueryInput.getEncryptionKey();

                setAuthenticationProperties(authSecurityProtocol, kafkaProperties, saslMechanism, userName, password);
                KafkaProducer<String, String> producer = new KafkaProducer<>(kafkaProperties);
                log.info( "creating the kafka instance");

//            if (encryptionType.equalsIgnoreCase(AES_ENCRYPTION)) {
//                responseNode = doOptionalMessageEncryption(responseNode, encryptionType, encryptionKey);
//            }

                ObjectNode objectNode = objectMapper.createObjectNode();
                objectNode.put("documentId", documentId);
                objectNode.put("checksum", fileChecksum);
                objectNode.put("transactionId", transactionId);
                objectNode.set("extractionResponse", productJson);

                String messageNode;
                try {
                    messageNode = objectMapper.writeValueAsString(objectNode);
                } catch (Exception e) {
                    log.error(aMarker, "Error in converting json data to kafka topic message", e);
                    insertExecutionInfo(jdbi, outputTable, documentId, fileChecksum, tenantId, originId, batchId, topicName, endpoint, authSecurityProtocol, saslMechanism, e.getMessage(), -1, FAILED_STATUS, transactionId);
                    throw new HandymanException("Error in converting json data to kafka topic message", e, action);
                }



                ProducerRecord<String, String> producerRecord = new ProducerRecord<>(topicName, messageNode);
                log.info( "sending the topic and outputjson");

                try {
                    producer.send(producerRecord, (metadata, exception) -> {
                        if (exception == null) {
                            log.info("Successful in sending the message to kafka topic");
                            int partition = metadata.partition();
                            log.info("Topic: {}, Partition: {}", metadata.topic(), partition);
                            insertExecutionInfo(jdbi, outputTable, documentId, fileChecksum, tenantId, originId, batchId, topicName, endpoint, authSecurityProtocol, saslMechanism, "Successful in sending message to topic", partition, "SUCCESS", transactionId);
                        } else {
                            log.error(aMarker, "Error in publishing message to kafka topic", exception);
                            insertExecutionInfo(jdbi, outputTable, documentId, fileChecksum, tenantId, originId, batchId, topicName, endpoint, authSecurityProtocol, saslMechanism, exception.getMessage(), -1, FAILED_STATUS, transactionId);
                            throw new HandymanException("Error in publishing message to kafka topic", exception, action);
                        }
                    });
                } finally {
                    producer.close();
                }
            } catch (HandymanException e) {
                insertExecutionInfo(jdbi, outputTable, documentId, fileChecksum, tenantId, originId, batchId, topicName, endpoint, authSecurityProtocol, saslMechanism, e.getMessage(), -1, FAILED_STATUS, transactionId);
                throw new HandymanException("Error in posting kafka topic message", e, action);
            }
        }

    }

    private void insertExecutionInfo(Jdbi jdbi, String outputTable, String documentId, String checksum, Long tenantId, String originId,
                                     String batchId, String topicName, String endpoint, String authSecurityProtocol,
                                     String saslMechanism, String response, int partition, String executionStatus, String transactionId) {

        jdbi.useHandle(handle -> handle.createUpdate("INSERT INTO product_outbound." + outputTable + "(document_id, checksum, tenant_id, origin_id, batch_id, topic_name, endpoint, auth_security_protocol, sasl_mechanism, response, partition, exec_status, transaction_id) " +
                        "VALUES(:documentId, :checksum, :tenantId, :originId, :batchId, :topicName, :endpoint, :authSecurityProtocol, :saslMechanism, :response, :partition, :execStatus, :transactionId);")
                .bind("documentId", documentId)
                .bind("checksum", checksum)
                .bind("tenantId", tenantId)
                .bind("originId", originId)
                .bind("batchId", batchId)
                .bind("topicName", topicName)
                .bind("endpoint", endpoint)
                .bind("authSecurityProtocol", authSecurityProtocol)
                .bind("saslMechanism", saslMechanism)
                .bind("response", response)
                .bind("partition", partition)
                .bind("execStatus", executionStatus)
                .bind("transactionId", transactionId)
                .execute());
    }

    private String doOptionalMessageEncryption(String messageNode, String encryptionType, String encryptionKey) throws NoSuchPaddingException, IllegalBlockSizeException, NoSuchAlgorithmException, BadPaddingException, InvalidKeyException {
        String message = EncryptDecrypt.encrypt(messageNode, encryptionKey, encryptionType);
        log.info("Encrypted message using algorithm {}", encryptionType);
        return message;

    }

    private static void setAuthenticationProperties(String authSecurityProtocol, Map<String, Object> properties, String saslMechanism, String userName, String password) {
        if (authSecurityProtocol.equalsIgnoreCase(SASL_SSL)) {
            properties.put("security.protocol", SASL_SSL);
            if (saslMechanism.equalsIgnoreCase(PLAIN_SASL)) {
                properties.put(SASL_MECHANISM, PLAIN_SASL);
                String jaasConfig = String.format("org.apache.kafka.common.security.plain.PlainLoginModule required username=\"%s\" password=\"%s\";", userName, password);
                properties.put("sasl.jaas.config", jaasConfig);
                properties.put("ssl.truststore.type", "JKS");
                properties.put("ssl.keystore.type", "JKS");

                properties.put("ssl.truststore.location", "/etc/kafka/secrets/kafka.truststore.jks");
                properties.put("ssl.truststore.password", "truststore-password");
                properties.put("ssl.keystore.location", "/etc/kafka/secrets/kafka.keystore.jks");
                properties.put("ssl.keystore.password", "keystore-password");
                properties.put("ssl.key.password", "keystore-password");
                properties.put("ssl.endpoint.identification.algorithm","");
                properties.put("acks", "all");
                properties.put("retries", 3);
                properties.put("request.timeout.ms", 30000);
                properties.put("delivery.timeout.ms", 300000);  // 5 minutes
                properties.put("linger.ms", 500);
            } else {
                if (saslMechanism.equalsIgnoreCase(SCRAM_SHA_256)) {
                    properties.put(SASL_MECHANISM, SCRAM_SHA_256);
                    String jaasConfig = String.format("org.apache.kafka.common.security.scram.ScramLoginModule required username=\"%s\" password=\"%s\";", userName, password);
                    properties.put("sasl.jaas.config", jaasConfig);
                } else {
                    if (saslMechanism.equalsIgnoreCase(OAUTH_BEARER)) {
                        properties.put(SASL_MECHANISM, OAUTH_BEARER);
                        properties.put("sasl.login.callback.handler.class", "your.oauth.LoginCallbackHandler");
                    }
                }
            }
        } else if (authSecurityProtocol.equalsIgnoreCase("SASL_PLAINTEXT")) {
            properties.put("security.protocol", "SASL_PLAINTEXT");
            properties.put(SASL_MECHANISM, PLAIN_SASL);
            String jaasConfig = String.format("org.apache.kafka.common.security.plain.PlainLoginModule required username=\"%s\" password=\"%s\";", userName, password);
            properties.put("sasl.jaas.config", jaasConfig);
//            properties.put("sasl.jaas.config",
//                    "org.apache.kafka.common.security.plain.PlainLoginModule required " +
//                            "username=\"" + userName + "\" " +
//                            "password=\"" + password + "\";");

        }
    }

    @Data
    @AllArgsConstructor
    @NoArgsConstructor
    @Builder
    public static class KafkaPublishQueryInput {
        private String endpoint;
        private String topicName;
        private String authSecurityProtocol;
        private String saslMechanism;
        private String userName;
        private String password;
        private String encryptionType;
        private String encryptionKey;
        private String jsonData;
        private String documentId;
        private String fileChecksum;
        private String originId;
        private String batchId;
        private Long tenantId;
        private String transactionId;
    }

    @Override
    public boolean executeIf() throws Exception {
        return kafkaPublish.getCondition();
    }
}
