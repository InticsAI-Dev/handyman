    package in.handyman.raven.lib;

    import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
    import com.fasterxml.jackson.databind.JsonNode;
    import com.fasterxml.jackson.databind.ObjectMapper;
    import com.jayway.jsonpath.Configuration;
    import com.jayway.jsonpath.JsonPath;
    import com.jayway.jsonpath.Option;
    import in.handyman.raven.exception.HandymanException;
    import in.handyman.raven.lambda.access.ResourceAccess;
    import in.handyman.raven.lambda.action.ActionExecution;
    import in.handyman.raven.lambda.action.IActionExecution;
    import in.handyman.raven.lambda.doa.audit.ActionExecutionAudit;
    import in.handyman.raven.lib.model.KafkaOutboundComparison;
    import in.handyman.raven.util.CommonQueryUtil;
    import lombok.*;
    import org.jdbi.v3.core.Jdbi;
    import org.jdbi.v3.core.mapper.ColumnMapper;
    import org.jdbi.v3.core.statement.Query;
    import org.jdbi.v3.core.statement.StatementContext;
    import org.slf4j.Logger;
    import org.slf4j.Marker;
    import org.slf4j.MarkerFactory;

    import java.sql.ResultSet;
    import java.sql.SQLException;
    import java.time.LocalDateTime;
    import java.util.*;
    import java.util.concurrent.atomic.AtomicInteger;
    import java.util.stream.Collectors;
    import java.util.stream.Stream;

    /**
     * Auto Generated By Raven
     */
    @ActionExecution(
            actionName = "KafkaOutboundComparison"
    )
    public class KafkaOutboundComparisonAction implements IActionExecution {
        private final ActionExecutionAudit action;
        private final Logger log;
        private final KafkaOutboundComparison kafkaOutboundComparison;
        private final Marker aMarker;
        private final Configuration jsonPathConfig = Configuration.defaultConfiguration()
                .addOptions(Option.SUPPRESS_EXCEPTIONS, Option.DEFAULT_PATH_LEAF_TO_NULL);

        public KafkaOutboundComparisonAction(final ActionExecutionAudit action, final Logger log,
                                             final Object jsonToTableConversion) {
            this.kafkaOutboundComparison = (KafkaOutboundComparison) jsonToTableConversion;
            this.action = action;
            this.log = log;
            this.aMarker = MarkerFactory.getMarker("JsonToTableConversion:" + this.kafkaOutboundComparison.getName());
        }

        @Override
        public void execute() throws Exception {
            Jdbi jdbi = null;
            try {
                jdbi = ResourceAccess.rdbmsJDBIConn(kafkaOutboundComparison.getResourceConn());
                jdbi.registerColumnMapper(new JsonNodeColumnMapper());
                String outputTableName = kafkaOutboundComparison.getOutputTable();

                if (outputTableName == null || outputTableName.trim().isEmpty()) {
                    log.error(aMarker, "Output table name is empty or null");
                    logAudit(jdbi, null, null, null, "ERROR", "Output table name is empty or null", null);
                    throw new HandymanException("Output table name is required");
                }

                Map<Long, SorFieldConfig> fieldConfigs = fetchSorFieldConfigs(jdbi);
                log.info(aMarker, "Fetched {} SOR field configurations", fieldConfigs.size());

                if (fieldConfigs.isEmpty()) {
                    log.error(aMarker, "No SOR field configurations found in sor_meta.field_wise_outbound_config");
                    logAudit(jdbi, null, null, null, "ERROR", "No SOR field configurations found", null);
                    throw new HandymanException("No SOR field configurations found");
                }

                // Create table with all sor_item_name columns
                ensureTableExists(jdbi, outputTableName, fieldConfigs);

                Jdbi finalJdbi = jdbi;
                jdbi.useTransaction(handle -> {
                    final List<String> formattedQuery = CommonQueryUtil.getFormattedQuery(kafkaOutboundComparison.getQuerySet());
                    AtomicInteger queryIndex = new AtomicInteger(0);

                    for (String sqlToExecute : formattedQuery) {
                        log.info(aMarker, "Executing query {} from index {}", queryIndex.get(), sqlToExecute);
                        Query query = handle.createQuery(sqlToExecute);

                        List<JsonToTableInput> inputs = query.mapToBean(JsonToTableInput.class)
                                .stream()
                                .peek(input -> log.debug(aMarker, "Fetched input: {}", input))
                                .collect(Collectors.toList());
                        log.info(aMarker, "Executed query from index {}, fetched {} records", queryIndex.getAndIncrement(), inputs.size());

                        if (inputs.isEmpty()) {
                            log.warn(aMarker, "No records retrieved for query at index {}. Skipping conversion.", queryIndex.get());
                            logAudit(finalJdbi, null, null, null, "WARNING", "No records retrieved for query at index " + queryIndex.get(), null);
                            continue;
                        }

                        for (JsonToTableInput input : inputs) {
                            processJsonRecord(finalJdbi, input, fieldConfigs, input.batchId, outputTableName);
                        }
                    }
                });

                log.info(aMarker, "JsonToTableConversion action completed");
                logAudit(jdbi, null, null, null, "SUCCESS", "JsonToTableConversion action completed", null);
                action.getContext().put(kafkaOutboundComparison.getName() + ".isSuccessful", "true");

            } catch (Exception e) {
                action.getContext().put(kafkaOutboundComparison.getName() + ".isSuccessful", "false");
                log.error(aMarker, "Error in JsonToTableConversion action", e);
                logAudit(jdbi, null, null, null, "ERROR", "JsonToTableConversion failed", e.getMessage());
                HandymanException handymanException = new HandymanException("Failed to convert JSON to table", e);
                HandymanException.insertException("Failed to convert JSON to table", handymanException, action);
                throw handymanException;
            }
        }

        private void processJsonRecord(Jdbi jdbi, JsonToTableInput input, Map<Long, SorFieldConfig> fieldConfigs,
                                       String batchId, String outputTableName) {
            try {
                String docId = input.getDocumentId();
                log.info(aMarker, "Processing JSON to table conversion for ID: {}", docId);

                JsonNode jsonResponse = input.getProductionResponse();
                if (jsonResponse == null) {
                    log.error(aMarker, "Null JSON response for ID: {}", docId);
                    logAudit(jdbi, input.getTransactionId(), input.getRequestTxnId(), docId, "ERROR",
                            "Null JSON response for ID: " + docId, null, batchId, input.getTenantId());
                    return;
                }

                // Convert JSON to table format using SOR configuration
                TableConversionResult conversionResult = convertJsonToTable(jsonResponse, fieldConfigs, docId);

                // Insert converted data into output table
                insertTableRecord(jdbi, input, conversionResult.getFieldValues(), batchId, outputTableName);

                // Log successful processing
                logAudit(jdbi, input.getTransactionId(), input.getRequestTxnId(), docId, conversionResult.getStatus(),
                        conversionResult.getMessage(), null, batchId, input.getTenantId());

            } catch (Exception e) {
                log.error(aMarker, "Error processing ID: {}", input != null ? input.getDocumentId() : "N/A", e);
                logAudit(jdbi, input != null ? input.getTransactionId() : null,
                        input != null ? input.getRequestTxnId() : null,
                        input != null ? input.getDocumentId() : null,
                        "ERROR", "Error processing ID: " + (input != null ? input.getDocumentId() : "N/A"),
                        e.getMessage(), batchId, input != null ? input.getTenantId() : null);
            }
        }

        private void logAudit(Jdbi jdbi, String transactionId, String requestTxnId, String documentId,
                              String status, String message, String errorDetails) {
            logAudit(jdbi, transactionId, requestTxnId, documentId, status, message, errorDetails, null, null);
        }

        private void logAudit(Jdbi jdbi, String transactionId, String requestTxnId, String documentId,
                              String status, String message, String errorDetails, String batchId, Long tenantId) {
            try {
                jdbi.useHandle(handle -> {
                    String sql = "INSERT INTO audit.kafka_outbound_comparison_audit " +
                            "(transaction_id, request_txn_id, document_id, batch_id, status, message, error_details, pipeline_name, tenant_id) " +
                            "VALUES (:transactionId, :requestTxnId, :documentId, :batchId, :status, :message, :errorDetails, :pipelineName, :tenantId)";

                    handle.createUpdate(sql)
                            .bind("transactionId", transactionId != null ? transactionId : "N/A")
                            .bind("requestTxnId", requestTxnId != null ? requestTxnId : "N/A")
                            .bind("documentId", documentId)
                            .bind("batchId", batchId)
                            .bind("status", status)
                            .bind("message", message)
                            .bind("errorDetails", errorDetails)
                            .bind("pipelineName", kafkaOutboundComparison.getName())
                            .bind("tenantId", tenantId)
                            .execute();
                });
                log.debug(aMarker, "Logged audit entry for transaction ID: {}, status: {}", transactionId, status);
            } catch (Exception e) {
                log.error(aMarker, "Failed to log audit entry for transaction ID: {}", transactionId, e);
            }
        }

        private TableConversionResult convertJsonToTable(JsonNode jsonData, Map<Long, SorFieldConfig> fieldConfigs,
                                                         String docId) {
            Map<String, Object> fieldValues = new HashMap<>();
            List<String> validationErrors = new ArrayList<>();
            String status = "SUCCESS";
            String message = "Successfully converted JSON to table format";

            for (SorFieldConfig fieldConfig : fieldConfigs.values()) {
                try {
                    String sorItemName = fieldConfig.getSorItemName().toLowerCase();

                    // Skip if already processed
                    if (fieldValues.containsKey(sorItemName)) {
                        continue;
                    }

                    // Extract value using JsonPath
                    String extractedValue = extractValueByJsonPath(jsonData, fieldConfig);

                    // Validate extracted value
                    ValidationResult validation = validateExtractedValue(extractedValue);
                    if (!validation.isValid()) {
                        validationErrors.add(validation.getErrorMessage());
                        status = "ERROR";
                        message = "Validation failed: " + validation.getErrorMessage();
                        continue;
                    }

                    fieldValues.put(sorItemName, extractedValue);

                } catch (Exception e) {
                    log.error(aMarker, "Error processing field {} for ID: {}", fieldConfig.getSorItemName(), docId, e);
                    validationErrors.add("Error processing field " + fieldConfig.getSorItemName() + ": " + e.getMessage());
                    status = "ERROR";
                    message = "Field processing error: " + e.getMessage();
                }
            }

            return TableConversionResult.builder()
                    .fieldValues(fieldValues)
                    .validationErrors(validationErrors)
                    .status(status)
                    .message(message)
                    .build();
        }

        private String extractValueByJsonPath(JsonNode jsonData, SorFieldConfig fieldConfig) {
            try {
                List<Object> allValues = new ArrayList<>();
                String[] jsonPaths = fieldConfig.getJsonPath().split("\\|");

                for (String jsonPath : jsonPaths) {
                    jsonPath = jsonPath.trim();
                    log.debug(aMarker, "Evaluating JsonPath: {} for field: {}", jsonPath, fieldConfig.getSorItemName());
                    Object value = JsonPath.using(jsonPathConfig).parse(jsonData.toString()).read(jsonPath);

                    if (value instanceof List) {
                        allValues.addAll((List<?>) value);
                    } else if (value != null && !value.toString().trim().isEmpty()) {
                        allValues.add(value);
                    }
                }

                // Remove empty or null values
                allValues.removeIf(v -> v == null || v.toString().trim().isEmpty());
                log.debug(aMarker, "Extracted values for {}: {}", fieldConfig.getSorItemName(), allValues);

                if (allValues.isEmpty()) {
                    log.debug(aMarker, "No valid values extracted for {}: returning empty string", fieldConfig.getSorItemName());
                    return "";
                }

                // Convert all values to a single string, joining with delimiter
                String delimiter = fieldConfig.isArray() && fieldConfig.getArrayDelimiter() != null
                        ? fieldConfig.getArrayDelimiter() : ",";
                String result = allValues.stream()
                        .flatMap(val -> val instanceof List
                                ? ((List<?>) val).stream().map(Object::toString)
                                : Stream.of(val.toString()))
                        .filter(s -> !s.isEmpty())
                        .collect(Collectors.joining(delimiter));

                log.debug(aMarker, "Final extracted value for {}: {}", fieldConfig.getSorItemName(), result);
                return result.isEmpty() ? "" : result;

            } catch (Exception e) {
                log.warn(aMarker, "Failed to extract value using JsonPath: {} for field: {}", fieldConfig.getJsonPath(), fieldConfig.getSorItemName(), e);
                return "";
            }
        }

        private ValidationResult validateExtractedValue(String value) {
            // Allow empty string for all fields, including required ones
            if (value == null || value.trim().isEmpty()) {
                return ValidationResult.builder()
                        .isValid(true)
                        .errorMessage("")
                        .build();
            }

            return ValidationResult.builder().isValid(true).errorMessage("").build();
        }

        private Map<Long, SorFieldConfig> fetchSorFieldConfigs(Jdbi jdbi) {
            return jdbi.withHandle(handle -> handle.createQuery(
                            "SELECT fw.sor_item_config_id AS id, fw.json_path, fw.sor_item_name, " +
                                    "fw.is_required, fw.is_array, fw.array_delimiter, " +
                                    "si.is_encrypted AS is_encrypt, ep.encryption_policy " +
                                    "FROM sor_meta.field_wise_outbound_config fw " +
                                    "JOIN sor_meta.sor_item si ON si.sor_item_id = fw.sor_item_id " +
                                    "JOIN sor_meta.encryption_policies ep ON ep.encryption_policy_id = si.encryption_policy_id " +
                                    "WHERE si.status = 'ACTIVE'")
                    .mapToBean(SorFieldConfig.class)
                    .stream()
                    .collect(Collectors.toMap(SorFieldConfig::getId, fc -> fc)));
        }

        private void ensureTableExists(Jdbi jdbi, String outputTableName, Map<Long, SorFieldConfig> fieldConfigs) {
            try {
                String schema = outputTableName.contains(".") ? outputTableName.split("\\.")[0] : "public";
                String table = outputTableName.contains(".") ? outputTableName.split("\\.")[1] : outputTableName;

                boolean tableExists = jdbi.withHandle(handle -> {
                    return handle.createQuery(
                                    "SELECT COUNT(*) FROM information_schema.tables " +
                                            "WHERE table_schema = :schema AND table_name = :table")
                            .bind("schema", schema)
                            .bind("table", table)
                            .mapTo(Integer.class)
                            .one() > 0;
                });

                if (!tableExists) {
                    log.info(aMarker, "Output table {} does not exist, creating it with all SOR columns", outputTableName);
                    jdbi.useHandle(handle -> {
                        StringBuilder columns = new StringBuilder("docid TEXT PRIMARY KEY");
                        for (SorFieldConfig config : fieldConfigs.values()) {
                            String columnName = config.getSorItemName().toLowerCase();
                            // Quote column name to handle special characters or reserved words
                            columns.append(", \"").append(columnName).append("\" VARCHAR");
                        }
                        String createTableSql = String.format("CREATE TABLE %s (%s)", outputTableName, columns);
                        log.info(aMarker, "Executing table creation: {}", createTableSql);
                        handle.execute(createTableSql);
                    });
                    log.info(aMarker, "Successfully created table {}", outputTableName);
                } else {
                    log.info(aMarker, "Output table {} already exists", outputTableName);
                    // Check for missing columns but do not alter
                    Set<String> existingColumns = jdbi.withHandle(handle -> handle.createQuery(
                                    "SELECT column_name FROM information_schema.columns " +
                                            "WHERE table_schema = :schema AND table_name = :table")
                            .bind("schema", schema)
                            .bind("table", table)
                            .mapTo(String.class)
                            .map(String::toLowerCase)
                            .collect(Collectors.toSet()));

                    Set<String> requiredColumns = fieldConfigs.values().stream()
                            .map(config -> config.getSorItemName().toLowerCase())
                            .collect(Collectors.toSet());
                    requiredColumns.add("docid");

                    List<String> missingColumns = requiredColumns.stream()
                            .filter(column -> !existingColumns.contains(column))
                            .collect(Collectors.toList());

                    if (!missingColumns.isEmpty()) {
                        log.warn(aMarker, "Table {} is missing columns: {}. No alterations will be made.", outputTableName, missingColumns);
                        logAudit(jdbi, null, null, null, "WARNING",
                                "Table " + outputTableName + " is missing columns: " + missingColumns, null);
                    }
                }
            } catch (Exception e) {
                log.error(aMarker, "Failed to create or verify table {}: {}", outputTableName, e.getMessage(), e);
                logAudit(jdbi, null, null, null, "ERROR", "Failed to ensure table existence for " + outputTableName, e.getMessage());
                throw new HandymanException("Failed to ensure table existence for " + outputTableName, e);
            }
        }

        private void insertTableRecord(Jdbi jdbi, JsonToTableInput input, Map<String, Object> fieldValues,
                                       String batchId, String outputTableName) {
            String docId = null;
            try {
                docId = input != null ? input.getDocumentId() : null;
                if (docId != null) {
                    String finalDocId1 = docId;
                    boolean docIdExists = jdbi.withHandle(handle -> {
                        return handle.createQuery(
                                        "SELECT COUNT(*) FROM " + outputTableName + " WHERE docid = :docid")
                                .bind("docid", finalDocId1)
                                .mapTo(Integer.class)
                                .one() > 0;
                    });

                    if (docIdExists) {
                        log.info(aMarker, "Skipping record for ID: {} as it already exists in table {}", docId, outputTableName);
                        logAudit(jdbi, input.getTransactionId(), input.getRequestTxnId(), docId, "SKIPPED",
                                "Record already exists for ID: " + docId, null, batchId, input.getTenantId());
                        return;
                    }
                } else {
                    log.warn(aMarker, "ID is null for input, skipping insertion into {}", outputTableName);
                    logAudit(jdbi, input != null ? input.getTransactionId() : null,
                            input != null ? input.getRequestTxnId() : null,
                            null, "WARNING", "ID is null, skipping insertion into " + outputTableName,
                            null, batchId, input != null ? input.getTenantId() : null);
                    return;
                }

                // Filter fieldValues to include only existing columns
                Set<String> existingColumns = jdbi.withHandle(handle -> {
                    String schema = outputTableName.contains(".") ? outputTableName.split("\\.")[0] : "public";
                    String table = outputTableName.contains(".") ? outputTableName.split("\\.")[1] : outputTableName;
                    return handle.createQuery(
                                    "SELECT column_name FROM information_schema.columns " +
                                            "WHERE table_schema = :schema AND table_name = :table")
                            .bind("schema", schema)
                            .bind("table", table)
                            .mapTo(String.class)
                            .map(String::toLowerCase)
                            .collect(Collectors.toSet());
                });

                Map<String, Object> filteredFieldValues = fieldValues.entrySet().stream()
                        .filter(entry -> existingColumns.contains(entry.getKey().toLowerCase()))
                        .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));

                if (filteredFieldValues.isEmpty() && !existingColumns.contains("docid")) {
                    log.warn(aMarker, "No valid columns to insert for ID: {} in table {}", docId, outputTableName);
                    logAudit(jdbi, input.getTransactionId(), input.getRequestTxnId(), docId, "WARNING",
                            "No valid columns to insert for ID: " + docId, null, batchId, input.getTenantId());
                    return;
                }

                String finalDocId = docId;
                jdbi.useHandle(handle -> {
                    StringBuilder columns = new StringBuilder("docid");
                    StringBuilder values = new StringBuilder(":docid");

                    for (String column : filteredFieldValues.keySet()) {
                        columns.append(", \"").append(column).append("\"");
                        values.append(", :").append(column);
                    }

                    String sql = String.format("INSERT INTO %s (%s) VALUES (%s)",
                            outputTableName, columns.toString(), values.toString());

                    var update = handle.createUpdate(sql)
                            .bind("docid", finalDocId);

                    for (Map.Entry<String, Object> entry : filteredFieldValues.entrySet()) {
                        update.bind(entry.getKey(), entry.getValue());
                    }

                    int rowsInserted = update.execute();
                    log.info(aMarker, "Inserted {} row(s) for ID: {}", rowsInserted, finalDocId);
                });
            } catch (Exception e) {
                log.error(aMarker, "Failed to insert record for ID: {}, error", docId != null ? docId : "N/A", e);
                logAudit(jdbi, input != null ? input.getTransactionId() : null,
                        input != null ? input.getRequestTxnId() : null,
                        docId, "ERROR", "Failed to insert record for ID: " + (docId != null ? docId : "N/A"),
                        e.getMessage(), batchId, input != null ? input.getTenantId() : null);
                throw new HandymanException("Failed to insert record into " + outputTableName, e);
            }
        }

        @Override
        public boolean executeIf() throws Exception {
            return kafkaOutboundComparison.getCondition();
        }

        @Data
        @Builder
        @NoArgsConstructor
        @AllArgsConstructor
        public static class SorFieldConfig {
            private Long id;
            private String jsonPath;
            private String sorItemName;
            private boolean isRequired;
            private boolean isArray;
            private String arrayDelimiter;
            private boolean isEncrypt;
            private String encryptionPolicy;
        }

        @Data
        @AllArgsConstructor
        @JsonIgnoreProperties(ignoreUnknown = true)
        public static class JsonToTableInput {
            private LocalDateTime createdOn;
            private Long createdUserId;
            private LocalDateTime lastUpdatedOn;
            private Long lastUpdatedUserId;
            private String status;
            private Integer version;
            private JsonNode productionResponse;
            private String documentId;
            private String extension;
            private String originId;
            private Long tenantId;
            private String transactionId;
            private String requestTxnId;
            private String filename;
            private String batchId;
            private String message;

            public JsonToTableInput() {
            }
        }

        @Data
        @Builder
        @NoArgsConstructor
        @AllArgsConstructor
        public static class ValidationResult {
            private boolean isValid;
            private String errorMessage;
        }

        @Data
        @Builder
        @NoArgsConstructor
        @AllArgsConstructor
        public static class TableConversionResult {
            private Map<String, Object> fieldValues;
            private List<String> validationErrors;
            private String status;
            private String message;
        }

        public static class JsonNodeColumnMapper implements ColumnMapper<JsonNode> {
            private static final ObjectMapper MAPPER = new ObjectMapper();

            @Override
            public JsonNode map(ResultSet rs, int columnNumber, StatementContext ctx) throws SQLException {
                String json = rs.getString(columnNumber);
                try {
                    return json != null ? MAPPER.readTree(json) : null;
                } catch (Exception e) {
                    throw new SQLException("Failed to parse JSON for column " + columnNumber, e);
                }
            }
        }
    }