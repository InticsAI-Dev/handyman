package in.handyman.raven.lib;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.SerializationFeature;
import com.fasterxml.jackson.databind.json.JsonMapper;
import com.fasterxml.jackson.datatype.jsr310.JavaTimeModule;
import in.handyman.raven.exception.HandymanException;
import in.handyman.raven.lambda.access.ResourceAccess;
import in.handyman.raven.lambda.action.ActionExecution;
import in.handyman.raven.lambda.action.IActionExecution;
import in.handyman.raven.lambda.doa.audit.ActionExecutionAudit;
import in.handyman.raven.lib.alchemy.common.AlchemyApiPayload;
import in.handyman.raven.lib.alchemy.common.BoundingBox;
import in.handyman.raven.lib.alchemy.common.Feature;
import in.handyman.raven.lib.encryption.SecurityEngine;
import in.handyman.raven.lib.model.AlchemyResponse;
import in.handyman.raven.util.CommonQueryUtil;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import okhttp3.*;
import org.jdbi.v3.core.Jdbi;
import org.jdbi.v3.core.argument.Arguments;
import org.jdbi.v3.core.argument.NullArgument;
import org.jdbi.v3.core.result.ResultIterable;
import org.jdbi.v3.core.statement.Query;
import org.jetbrains.annotations.NotNull;
import org.slf4j.Logger;
import org.slf4j.Marker;
import org.slf4j.MarkerFactory;

import java.net.MalformedURLException;
import java.net.URL;
import java.sql.Types;
import java.time.LocalDateTime;
import java.util.*;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.stream.Collectors;

/**
 * Auto Generated By Raven
 */
@ActionExecution(
        actionName = "AlchemyResponse"
)
public class AlchemyResponseAction implements IActionExecution {
    public static final String ALCHEMY_TRANSFORM = "ALCHEMY_TRANSFORM";
    private final ActionExecutionAudit action;

    private final Logger log;

    private final AlchemyResponse alchemyResponse;

    private final Marker aMarker;
    public static final String PIPELINE_REQ_RES_ENCRYPTION = "pipeline.req.res.encryption";

    private static final int DEFAULT_BATCH_SIZE = 50;

    private static final String DEFAULT_CONSUMER_API_COUNT = "10";

    final OkHttpClient httpclient = new OkHttpClient.Builder()
            .connectTimeout(10, TimeUnit.MINUTES)
            .writeTimeout(10, TimeUnit.MINUTES)
            .readTimeout(10, TimeUnit.MINUTES)
            .build();

    public AlchemyResponseAction(final ActionExecutionAudit action, final Logger log, final Object alchemyResponse) {
        this.alchemyResponse = (AlchemyResponse) alchemyResponse;
        this.action = action;
        this.log = log;
        this.aMarker = MarkerFactory.getMarker(" AlchemyResponse:" + this.alchemyResponse.getName());
    }

    @Override
    public void execute() throws Exception {
        try {
            final Jdbi jdbi = ResourceAccess.rdbmsJDBIConn(alchemyResponse.getResourceConn());
            jdbi.getConfig(Arguments.class).setUntypedNullArgument(new NullArgument(Types.NULL));
            int consumerApiCount = getConsumerApiCount();
            Long tenantId = getTenantId();
            URL url = getOriginValuationUrl();
            String authToken = getAlchemyAuthToken();
            ObjectMapper mapper = new ObjectMapper();
            MediaType mediaTypeJSON = MediaType.parse("application/json; charset=utf-8");

            log.info(aMarker, "Alchemy Response Action for {} has been started", alchemyResponse.getName());

            List<AlchemyResponseInputTable> tableInfos = getDataFromSelectQuery(jdbi);

            Map<String, List<AlchemyResponseInputTable>> groupedByOriginId = getOriginBasedPredictions(tableInfos);
            for (Map.Entry<String, List<AlchemyResponseInputTable>> entry : groupedByOriginId.entrySet()) {
                String originId = entry.getKey();
                processInBatchesParallel(url, tableInfos, tenantId, authToken, mapper, mediaTypeJSON, jdbi, DEFAULT_BATCH_SIZE, originId, httpclient, consumerApiCount);
            }

        } catch (Exception t) {
            handleExecutionError(t);
        }
    }


    private void handleExecutionError(Exception e) {
        action.getContext().put(alchemyResponse.getName() + ".isSuccessful", "false");
        log.error(aMarker, "Error in AlchemyResponse execution", e);
        throw new HandymanException("Error in AlchemyResponse execution", e, action);
    }

    private List<AlchemyResponseInputTable> getDataFromSelectQuery(Jdbi jdbi) {
        List<AlchemyResponseInputTable> tableInfos = new ArrayList<>();
        jdbi.useTransaction(handle -> {
            final List<String> formattedQuery = CommonQueryUtil.getFormattedQuery(alchemyResponse.getQuerySet());
            AtomicInteger i = new AtomicInteger(0);
            formattedQuery.forEach(sqlToExecute -> {
                log.info(aMarker, "executing  query {} from index {}", sqlToExecute, i.getAndIncrement());
                Query query = handle.createQuery(sqlToExecute);
                ResultIterable<AlchemyResponseInputTable> resultIterable = query.mapToBean(AlchemyResponseInputTable.class);
                List<AlchemyResponseInputTable> detailList = resultIterable.stream().collect(Collectors.toList());
                tableInfos.addAll(detailList);
                log.info(aMarker, "executed query from index {}", i.get());
            });
        });
        return tableInfos;
    }

    private String getAlchemyAuthToken() {
        String authToken = action.getContext().get("alchemyAuth.token");
        log.info("Getting the auth token from context");
        return authToken;
    }

    @NotNull
    private URL getOriginValuationUrl() throws MalformedURLException {
        URL url = new URL(action.getContext().get("alchemy.origin.valuation.url"));
        log.info("Getting valuation URL for prediction list insert - {}", url);
        return url;
    }

    @NotNull
    private Long getTenantId() {
        Long tenantId = Long.valueOf(action.getContext().get("alchemyAuth.tenantId"));
        log.info("Getting Tenant Id: {} for the user", tenantId);
        return tenantId;
    }

    private int getConsumerApiCount() {
        String consumerApiCountStr = action.getContext().getOrDefault("alchemy.response.consumer.API.count", DEFAULT_CONSUMER_API_COUNT);
        int consumerApiCount = Integer.parseInt(consumerApiCountStr);
        log.info("alchemy.response.consumer.API.count is {}", consumerApiCount);
        return consumerApiCount;
    }

    @Override
    public boolean executeIf() throws Exception {
        return alchemyResponse.getCondition();
    }

    public void processInBatchesParallel(URL endpoint, List<AlchemyResponseInputTable> inputTables,
                                         Long tenantId,
                                         String authToken,
                                         ObjectMapper objectMapper,
                                         MediaType mediaType,
                                         Jdbi jdbi,
                                         int batchSize,
                                         String originId,
                                         OkHttpClient httpClient,
                                         int maxParallelThreads) {

        int totalSize = inputTables.size();
        int totalBatches = (int) Math.ceil((double) totalSize / batchSize);
        log.info("Batch size is {}", DEFAULT_BATCH_SIZE);
        log.info("Total batches for prediction insert for originId: {} is: {}", originId, totalBatches);

        ExecutorService executor = Executors.newFixedThreadPool(Math.min(maxParallelThreads, totalBatches));
        List<CompletableFuture<Void>> futures = new ArrayList<>();

        for (int batchIndex = 0; batchIndex < totalBatches; batchIndex++) {
            final int start = batchIndex * batchSize;
            final int end = Math.min(start + batchSize, totalSize);
            final int batchNumber = batchIndex + 1;

            CompletableFuture<Void> future = CompletableFuture.runAsync(() -> {
                try {
                    List<AlchemyResponseInputTable> batch = inputTables.subList(start, end);
                    List<AlchemyRequestBody> requestData = new ArrayList<>(batch.size());

                    for (AlchemyResponseInputTable input : batch) {
                        requestData.add(buildAlchemyRequest(input, objectMapper));
                    }
                    log.info("Processing batch {}/{} with size: {}", batchNumber, totalBatches, requestData.size());

                    AlchemyResponseOutputTable alchemyResponseOutputTable = new AlchemyResponseOutputTable();
                    executeAlchemyPredictionApi(endpoint, originId, tenantId, authToken, requestData, objectMapper, mediaType, httpClient, alchemyResponseOutputTable);

                    consumerBatch(jdbi, alchemyResponseOutputTable);

                } catch (Exception e) {
                    log.error("Error processing batch {} for originId: {}", batchNumber, originId, e);
                    HandymanException handymanException = new HandymanException(e);
                    HandymanException.insertException("Error processing prediction insert request for alchemy for batch {} for originId: {}", handymanException, this.action);
                }
            }, executor);

            futures.add(future);
        }

        try {
            CompletableFuture.allOf(futures.toArray(new CompletableFuture[0])).join();

            for (CompletableFuture<?> future : futures) {
                future.get();
            }
        } catch (Exception e) {
            log.error("Batch processing failed for originId: {}", originId, e);
            HandymanException handymanException = new HandymanException(e);
            HandymanException.insertException("Batch processing failed for processing prediction insert request for alchemy for batch {} for originId: {}", handymanException, this.action);
        } finally {
            executor.shutdown();
            try {
                if (!executor.awaitTermination(60, TimeUnit.SECONDS)) {
                    executor.shutdownNow();
                }
            } catch (InterruptedException e) {
                executor.shutdownNow();
                Thread.currentThread().interrupt();
            }
        }
    }


    private AlchemyRequestBody buildAlchemyRequest(AlchemyResponseInputTable input, ObjectMapper objectMapper) {
        AlchemyRequestBody request = AlchemyRequestBody.builder()
                .paperNo(input.getPaperNo())
                .rootPipelineId(input.getRootPipelineId())
                .feature(input.getFeature())
                .build();

        try {
            switch (Feature.valueOf(input.getFeature())) {
                case KIE:
                case CHECKBOX_EXTRACTION:
                    request.setBbox(objectMapper.readTree(input.getBbox()));
                    request.setConfidenceScore(input.getConfidenceScore());
                    request.setExtractedValue(input.getExtractedValue());
                    request.setSynonymId(input.getSynonymId());
                    request.setQuestionId(input.getQuestionId());
                    request.setBatchId(input.getBatchId());
                    break;

                case TABLE_EXTRACT:
                    request.setTableData(objectMapper.readTree(input.getTableData()));
                    request.setCsvFilePath(input.getCsvFilePath());
                    request.setTruthEntityId(input.getTruthEntityId());
                    break;

                case CURRENCY_DETECTION:
                    request.setDetectedValue(input.getDetectedValue());
                    request.setDetectedAsciiValue(input.getDetectedAsciiValue());
                    request.setConfidenceScore(input.getConfidenceScore());
                    break;

                case TABLE_EXTRACT_AGGREGATE:
                    request.setTableData(objectMapper.readTree(input.getTableData()));
                    request.setAggregateJson(objectMapper.readTree(input.getTableAggregateNode()));
                    request.setSorItemId(input.getSorItemId());
                    break;

                case BULLETIN_EXTRACTION:
                    request.setBulletinPoints(objectMapper.readTree(input.getBulletinPoints()));
                    request.setBulletinSection(input.getBulletinSection());
                    request.setSynonymId(input.getSynonymId());
                    break;

                case PARAGRAPH_EXTRACTION:
                    request.setParagraphPoints(objectMapper.readTree(input.getParagraphPoints()));
                    request.setParagraphSection(input.getParagraphSection());
                    request.setSynonymId(input.getSynonymId());
                    break;

                case FACE_DETECTION:
                    request.setBbox(buildBoundingBox(input, objectMapper));
                    request.setEncode(input.getEncode());
                    request.setConfidenceScore(input.getConfidenceScore());
                    break;

                case FIGURE_DETECTION:
                case DOCUMENT_PARSER:
                    request.setEncode(input.getEncode());
                    break;
            }
        } catch (JsonProcessingException e) {
            throw new HandymanException("Error processing JSON for feature: " + input.getFeature(), e, action);
        }

        return request;
    }

    private JsonNode buildBoundingBox(AlchemyResponseInputTable input, ObjectMapper objectMapper) throws JsonProcessingException {
        BoundingBox bbox = new BoundingBox();
        bbox.setTopLeftX(Integer.parseInt(input.getLeftPos()));
        bbox.setTopLeftY(Integer.parseInt(input.getUpperPos()));
        bbox.setBottomRightX(Integer.parseInt(input.getRightPos()));
        bbox.setBottomRightY(Integer.parseInt(input.getLowerPos()));

        return objectMapper.readTree(objectMapper.writeValueAsString(bbox));
    }

    private void executeAlchemyPredictionApi(
            URL endpoint,
            String originId,
            Long tenantId,
            String authToken,
            List<AlchemyRequestBody> requestData,
            ObjectMapper objectMapper,
            MediaType mediaType,
            OkHttpClient httpClient,
            AlchemyResponseOutputTable alchemyResponseOutputTable
    ) {

        alchemyResponseOutputTable.setOriginId(originId);
        alchemyResponseOutputTable.setTenantId(tenantId);
        alchemyResponseOutputTable.setGroupId(getGroupIdFromContext());
        alchemyResponseOutputTable.setRootPipelineId(action.getRootPipelineId());
        alchemyResponseOutputTable.setEndpoint(endpoint.toString());
        try {

            String url = endpoint + "/" + originId + "/?tenantId=" + tenantId;
            String requestBodyStr = objectMapper.writeValueAsString(requestData);
            int predictionSize = requestData.size();
            log.info("Current batch size of predictions for originId: {}, is: {}", originId, predictionSize);
            RequestBody body = RequestBody.create(requestBodyStr, mediaType);
            alchemyResponseOutputTable.setRequest(encryptRequestResponse(requestBodyStr));
            alchemyResponseOutputTable.setCreatedOn(LocalDateTime.now());
            Request request = new Request.Builder()
                    .url(url)
                    .addHeader("accept", "*/*")
                    .addHeader("Authorization", "Bearer " + authToken)
                    .addHeader("Content-Type", "application/json")
                    .post(body)
                    .build();

            if (log.isInfoEnabled()) {
                log.info(aMarker, "Sending request to {} for originId {}", endpoint, originId);
            }

            try (Response response = httpClient.newCall(request).execute()) {
                if (response.isSuccessful()) {
                    alchemyResponseOutputTable.setStatus("COMPLETED");
                    alchemyResponseOutputTable.setStage(ALCHEMY_TRANSFORM);
                    alchemyResponseOutputTable.setCompletedOn(LocalDateTime.now());
                    String responseBody = Objects.requireNonNull(response.body()).string();

                    final ObjectMapper mapper = JsonMapper.builder()
                            .configure(SerializationFeature.INDENT_OUTPUT, true)
                            .disable(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS)
                            .build().registerModule(new JavaTimeModule());

                    AlchemyApiPayload alchemyApiPayload = mapper.readValue(responseBody, AlchemyApiPayload.class);
                    JsonNode payload = alchemyApiPayload.getPayload();

                    JsonNode predictionLogNode = payload.get("predictionLog");
                    if (predictionLogNode != null && predictionLogNode.isArray()) {
                        List<String> responseLog = mapper.convertValue(predictionLogNode, List.class);
                        String combinedLogs = String.join("\n", responseLog);
                        log.info("Logs from Alchemy for Prediction save: \n{}", combinedLogs);
                    }
                    alchemyResponseOutputTable.setResponse(encryptRequestResponse(String.valueOf(payload.get("predictions"))));
                    log.info("Response code: {}, Headers: {}", response.code(), response.headers());
                    log.info("Successful in saving the predictions of size {}, for originId: {}", predictionSize, originId);
                } else {
                    alchemyResponseOutputTable.setStatus("FAILED");
                    alchemyResponseOutputTable.setStage(ALCHEMY_TRANSFORM);
                    alchemyResponseOutputTable.setCompletedOn(LocalDateTime.now());
                    alchemyResponseOutputTable.setResponse(encryptRequestResponse(response.message()));
                    log.error("Request failed with status: {}", response.code());
                }
            }
        } catch (Exception e) {
            log.error(aMarker, "Exception during Alchemy request for originId {}", originId, e);
            HandymanException handymanException = new HandymanException(e);
            alchemyResponseOutputTable.setCompletedOn(LocalDateTime.now());
            alchemyResponseOutputTable.setStatus("FAILED");
            alchemyResponseOutputTable.setStage(ALCHEMY_TRANSFORM);
            alchemyResponseOutputTable.setResponse(encryptRequestResponse(e.getMessage()));
            HandymanException.insertException("Error in Alchemy request for originId - " + originId, handymanException, this.action);
        }


    }

    @NotNull
    private Long getGroupIdFromContext() {
        return Long.valueOf(action.getContext().get("group_id"));
    }

    void consumerBatch(final Jdbi jdbi, AlchemyResponseOutputTable resultQueue) {
        String outputTableName = getOutputTableName();
        try {
            jdbi.useTransaction(handle -> {
                try {
                    handle.createUpdate("INSERT INTO " + outputTableName +
                                    "(origin_id, root_pipeline_id, tenant_id, group_id, request, response, endpoint, status, stage, created_on, completed_on) " +
                                    "VALUES (:originId, :rootPipelineId, :tenantId, :groupId, :request, :response, :endpoint, :status, :stage, :createdOn, :completedOn);")
                            .bindBean(resultQueue)
                            .execute();
                    log.info("Inserted {} into alchemy_response_output_table", resultQueue.getOriginId());
                } catch (Throwable t) {

                    log.error("Error inserting result {}", resultQueue, t);
                }
            });
        } catch (Exception e) {
            log.error("Error inserting result {}", resultQueue, e);
            HandymanException handymanException = new HandymanException(e);
            HandymanException.insertException("Error inserting result " + resultQueue, handymanException, action);
        }
    }

    private String getOutputTableName() {
        String outputTableName = action.getContext().get("alchemy.response.output.table");
        log.info("Getting output table for alchemy prediction response - {}", outputTableName);
        return outputTableName;
    }

    @NotNull
    private Map<String, List<AlchemyResponseInputTable>> getOriginBasedPredictions(List<AlchemyResponseInputTable> entities) {
        Map<String, List<AlchemyResponseInputTable>> groupedByOriginId = entities.stream()
                .collect(Collectors.groupingBy(AlchemyResponseInputTable::getOriginId));
        log.info("Getting the predictions list grouped by origin");
        return groupedByOriginId;
    }

    @AllArgsConstructor
    @NoArgsConstructor
    @Data
    @Builder
    public static class AlchemyResponseOutputTable {
        private String originId;
        private Long rootPipelineId;
        private Long tenantId;
        private Long groupId;
        private String request;
        private String response;
        private String endpoint;
        private String status;
        private String stage;
        private LocalDateTime createdOn;
        private LocalDateTime completedOn;
    }


    @AllArgsConstructor
    @NoArgsConstructor
    @Data
    @Builder
    public static class AlchemyResponseInputTable implements CoproProcessor.Entity {

        private String originId;
        private Integer paperNo;
        private Long tenantId;
        private Long rootPipelineId;
        private Integer confidenceScore;
        private String extractedValue;
        private String sorItemName;
        private Long synonymId;
        private Long questionId;
        private String bbox;
        private String feature;
        private String state;
        private String tableData;
        private String detectedValue;
        private String detectedAsciiValue;
        private String csvFilePath;
        private Long truthEntityId;
        private String batchId;
        private String tableAggregateNode;
        private Long sorItemId;
        private String bulletinSection;
        private String bulletinPoints;
        private String paragraphSection;
        private String paragraphPoints;
        private String encode;
        private String leftPos;
        private String upperPos;
        private String rightPos;
        private String lowerPos;


        @Override
        public List<Object> getRowData() {
            return Collections.emptyList();
        }
    }

    @AllArgsConstructor
    @NoArgsConstructor
    @Data
    @Builder
    public static class AlchemyCurrencyResponse {
        private String detectedValue;
        private String detectedAsciiValue;
        private String confidenceScore;
    }

    @AllArgsConstructor
    @NoArgsConstructor
    @Data
    @Builder
    public static class AlchemyRequestBody {
        private Integer paperNo;
        private Integer confidenceScore;
        private String extractedValue;
        private JsonNode bbox;
        private Long synonymId;
        private Long questionId;
        private Long rootPipelineId;
        private String feature;
        private String state;
        private String csvFilePath;
        private Long truthEntityId;
        private JsonNode tableData;
        private String detectedValue;
        private String detectedAsciiValue;
        private String batchId;
        private JsonNode aggregateJson;
        private Long sorItemId;
        private String bulletinSection;
        private JsonNode bulletinPoints;
        private String paragraphSection;
        private JsonNode paragraphPoints;
        private String encode;
    }


    public String encryptRequestResponse(String request) {
        String encryptReqRes = action.getContext().get(PIPELINE_REQ_RES_ENCRYPTION);
        String requestStr;
        if ("true".equals(encryptReqRes)) {
            String encryptedRequest = SecurityEngine.getInticsIntegrityMethod(action).encrypt(request, "AES256", "COPRO_REQUEST");
            requestStr = encryptedRequest;
        } else {
            requestStr = request;
        }
        return requestStr;
    }

}
