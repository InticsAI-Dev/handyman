package in.handyman.raven.lib;

import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import in.handyman.raven.exception.HandymanException;
import in.handyman.raven.lambda.access.ResourceAccess;
import in.handyman.raven.lambda.action.ActionExecution;
import in.handyman.raven.lambda.action.IActionExecution;
import in.handyman.raven.lambda.doa.audit.ActionExecutionAudit;
import in.handyman.raven.lib.model.DataExtraction;
import in.handyman.raven.lib.model.TextExtraction.*;
import in.handyman.raven.lib.model.triton.TritonInputRequest;
import in.handyman.raven.lib.model.triton.TritonRequest;
import in.handyman.raven.util.ExceptionUtil;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import okhttp3.*;
import org.jdbi.v3.core.Jdbi;
import org.jdbi.v3.core.argument.Arguments;
import org.jdbi.v3.core.argument.NullArgument;
import org.json.JSONObject;
import org.slf4j.Logger;
import org.slf4j.Marker;
import org.slf4j.MarkerFactory;

import java.io.File;
import java.net.MalformedURLException;
import java.net.URL;
import java.sql.Timestamp;
import java.sql.Types;
import java.time.LocalDateTime;
import java.util.*;
import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.TimeUnit;
import java.util.stream.Collectors;

/**
 * Auto Generated By Raven
 */
@ActionExecution(actionName = "DataExtraction")
public class DataExtractionAction implements IActionExecution {
  private final ActionExecutionAudit action;

  private final Logger log;

  private final DataExtraction dataExtraction;
  private static final MediaType MediaTypeJSON = MediaType.parse("application/json; charset=utf-8");
  private final Marker aMarker;
  private final ObjectMapper mapper = new ObjectMapper();
  private static String httpClientTimeout = new String();

  private final String URI;



  public DataExtractionAction(final ActionExecutionAudit action, final Logger log, final Object dataExtraction) {
    this.dataExtraction = (DataExtraction) dataExtraction;
    this.action = action;
    this.log = log;
    this.aMarker = MarkerFactory.getMarker(" DataExtraction:" + this.dataExtraction.getName());
    this.URI = action.getContext().get("copro.data-extraction.url");
    this.httpClientTimeout=action.getContext().get("okhttp.client.timeout");
  }

  @Override
  public void execute() throws Exception {
    try{
      final Jdbi jdbi = ResourceAccess.rdbmsJDBIConn(dataExtraction.getResourceConn());

      jdbi.getConfig(Arguments.class).setUntypedNullArgument(new NullArgument(Types.NULL));
      log.info(aMarker, "Data Extraction Action for {} has been started", dataExtraction.getName());

      final String insertQuery = "INSERT INTO info.data_extraction(origin_id,group_id,tenant_id,template_id,process_id, file_path, extracted_text,paper_no,file_name, status,stage,message,is_blank_page, created_on ,root_pipeline_id,template_name,model_name,model_version) " + "" +
              " VALUES(?,? ,?,?,? ,?,?,?,?, ?,?,?,?,? ,?, ?,?,?)";
      final List<URL> urls = Optional.ofNullable(action.getContext().get("copro.data-extraction.url")).map(s -> Arrays.stream(s.split(",")).map(s1 -> {
        try {
          return new URL(s1);
        } catch (MalformedURLException e) {
          log.error("Error in processing the URL ", e);
          throw new HandymanException("Error in processing the URL",e, action);
        }
      }).collect(Collectors.toList())).orElse(Collections.emptyList());

      final CoproProcessor<DataExtractionInputTable, DataExtractionOutputTable> coproProcessor = new CoproProcessor<>(new LinkedBlockingQueue<>(), DataExtractionOutputTable.class, DataExtractionInputTable.class, jdbi, log, new DataExtractionInputTable(), urls, action);
      coproProcessor.startProducer(dataExtraction.getQuerySet(), Integer.valueOf(action.getContext().get("read.batch.size")));
      Thread.sleep(1000);
      coproProcessor.startConsumer(insertQuery, Integer.valueOf(action.getContext().get("text.extraction.consumer.API.count")), Integer.valueOf(action.getContext().get("write.batch.size")), new DataExtractionConsumerProcess(log, aMarker, action));
      log.info(aMarker, " Data Extraction Action has been completed {}  ", dataExtraction.getName());
    }catch (Exception e){
      action.getContext().put(dataExtraction.getName() + ".isSuccessful", "false");
      log.error(aMarker,"error in execute method in data extraction", e);
      throw new HandymanException("error in execute method in data extraction", e, action);
    }

  }

  public static class DataExtractionConsumerProcess implements CoproProcessor.ConsumerProcess<DataExtractionInputTable,DataExtractionOutputTable> {
    private final Logger log;
    private final Marker aMarker;
    private final ObjectMapper mapper = new ObjectMapper();
    private static final MediaType MediaTypeJSON = MediaType.parse("application/json; charset=utf-8");

    public final ActionExecutionAudit action;
    final OkHttpClient httpclient = new OkHttpClient.Builder()
            .connectTimeout(Long.parseLong(httpClientTimeout), TimeUnit.MINUTES)
            .writeTimeout(Long.parseLong(httpClientTimeout), TimeUnit.MINUTES)
            .readTimeout(Long.parseLong(httpClientTimeout), TimeUnit.MINUTES)
            .build();
    public DataExtractionConsumerProcess(final Logger log, final Marker aMarker, ActionExecutionAudit action) {
      this.log = log;
      this.aMarker = aMarker;
      this.action = action;
    }


    @Override
    public List<DataExtractionOutputTable> process(URL endpoint, DataExtractionInputTable entity) throws JsonProcessingException {
      List<DataExtractionOutputTable> parentObj = new ArrayList<>();
      String inputFilePath = entity.getFilePath();
      final String DATA_EXTRACTION_PROCESS="DATA_EXTRACTION";
      Long rootpipelineId= entity.getRootPipelineId();
      Long actionId=action.getActionId();
      String process = String.valueOf(entity.process);
      String filePath = String.valueOf(entity.getFilePath());
      ObjectMapper objectMapper = new ObjectMapper();


      //payload
      DataExtractionData DataExtractiondata = new DataExtractionData();
      DataExtractiondata.setRootPipelineId(Long.valueOf(rootpipelineId));
      DataExtractiondata.setActionId(actionId);
      DataExtractiondata.setProcess(process);
      DataExtractiondata.setInputFilePath(filePath);
      String jsonInputRequest = objectMapper.writeValueAsString(DataExtractiondata);



      DataExtractionRequest requests = new  DataExtractionRequest();
      TritonRequest requestBody = new TritonRequest();
      requestBody.setName("TEXT EXTRACTOR START");
      requestBody.setShape(List.of(1, 1));
      requestBody.setDatatype("BYTES");
      requestBody.setData(Collections.singletonList(jsonInputRequest));

      // requestBody.setData(Collections.singletonList(jsonNodeRequest));

      //  requestBody.setData(Collections.singletonList(DataExtractiondata));

      TritonInputRequest tritonInputRequest=new TritonInputRequest();
      tritonInputRequest.setInputs(Collections.singletonList(requestBody));

      String jsonRequest = objectMapper.writeValueAsString(requests);

      Request request = new Request.Builder().url(endpoint).post(RequestBody.create(jsonRequest.toString(), MediaTypeJSON)).build();

      if(log.isInfoEnabled()) {
        log.info(aMarker, "Request has been build with the parameters \n URI : {}, with inputFilePath {} ", endpoint, inputFilePath);
      }

      String originId = entity.getOriginId();
      Integer groupId = entity.getGroupId();

      try (Response response = httpclient.newCall(request).execute()) {
        String responseBody = Objects.requireNonNull(response.body()).string();
        if (response.isSuccessful()) {
          ObjectMapper objectMappers = new ObjectMapper();
         DataExtractionResponse modelResponse = objectMappers.readValue(responseBody, DataExtractionResponse.class);
         if (modelResponse.getOutputs() != null && !modelResponse.getOutputs().isEmpty()) {
           modelResponse.getOutputs().forEach(o -> {
             o.getData().forEach(DataExtractionDataItem -> {

               JSONObject parentResponseObject = new JSONObject(responseBody);
               final String contentString=Optional.ofNullable(parentResponseObject.get("pageContent")).map(String::valueOf).orElse(null);
               final String flag=(!Objects.isNull(contentString) && contentString.length() > 5 ) ? "no" : "yes";

               parentObj.add(DataExtractionOutputTable.builder()
                       .filePath(new File(entity.getFilePath()).getAbsolutePath())
                       .extractedText(contentString)
                       .originId(Optional.ofNullable(originId).map(String::valueOf).orElse(null))
                       .groupId(groupId)
                       .fileName(Optional.ofNullable(parentResponseObject.get("fileName")).map(String::valueOf).orElse(null))
                       .paperNo(entity.getPaperNo())
                       .status("COMPLETED")
                       .stage("DATA_EXTRACTION")
                       .message("Data extraction macro completed")
                       .createdOn(Timestamp.valueOf(LocalDateTime.now()))
                       .tenantId(entity.getTenantId())
                       .templateId(entity.getTemplateId())
                       .processId(entity.getProcessId())
                       .isBlankPage(flag)
                       .templateName(entity.getTemplateName())
                       .rootPipelineId(entity.getRootPipelineId())
                       .build());
             });
           });
         }
        }else{
          parentObj.add(DataExtractionOutputTable.builder()
                  .originId(Optional.ofNullable(originId).map(String::valueOf).orElse(null))
                  .groupId(groupId)
                  .paperNo(entity.getPaperNo())
                  .status("FAILED")
                  .stage("DATA_EXTRACTION")
                  .tenantId(entity.getTenantId())
                  .templateId(entity.getTemplateId())
                  .processId(entity.getProcessId())
                  .message(response.message())
                  .createdOn(Timestamp.valueOf(LocalDateTime.now()))
                  .rootPipelineId(entity.getRootPipelineId())
                  .templateName(entity.getTemplateName())
                  .build());
          log.error(aMarker, "The Exception occurred ");
        }
      } catch (Exception e) {
        parentObj.add(DataExtractionOutputTable.builder()
                .originId(Optional.ofNullable(originId).map(String::valueOf).orElse(null))
                .groupId(groupId)
                .paperNo(entity.getPaperNo())
                .status("FAILED")
                .stage("DATA_EXTRACTION")
                .tenantId(entity.getTenantId())
                .templateId(entity.getTemplateId())
                .processId(entity.getProcessId())
                .createdOn(Timestamp.valueOf(LocalDateTime.now()))
                .message(ExceptionUtil.toString(e))
                .rootPipelineId(entity.getRootPipelineId())
                .templateName(entity.getTemplateName())
                .build());

        log.error(aMarker, "The Exception occurred ", e);
        HandymanException handymanException = new HandymanException(e);
        HandymanException.insertException("test extraction consumer failed for batch/group "+ groupId, handymanException, this.action);

      }
      return parentObj;
    }

  }

  @Override
  public boolean executeIf() throws Exception {
    return dataExtraction.getCondition();
  }

  @Data
  @AllArgsConstructor
  @Builder
  @NoArgsConstructor
  @JsonIgnoreProperties(ignoreUnknown = true)
  public static class AssetAttributionResponse {
    private String pageContent;
  }

}